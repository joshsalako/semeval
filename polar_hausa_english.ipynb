{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshsalako/semeval/blob/main/polar_hausa_english.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
      "metadata": {
        "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697"
      },
      "source": [
        "# POLAR Subtask 1: Hausa-English Implementation\n",
        "\n",
        "This notebook implements the polarization detection task (Subtask 1) for both English and Hausa languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-setup"
      },
      "outputs": [],
      "source": [
        "model_name = \"castorini/afriteva_base\" #@param\n",
        "#\"Davlan/afro-xlmr-large\" # \"microsoft/mdeberta-v3-base\" Tadesse/AfroXLMR-Social"
      ],
      "id": "model-setup"
    },
    {
      "cell_type": "markdown",
      "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
      "metadata": {
        "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46cc22d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46cc22d9",
        "outputId": "240d0a46-0246-407a-ed34-708d3ae192a3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/608.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "! pip install emoji\n",
        "! pip install peft accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
      "metadata": {
        "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "70a2a2f8-bc36-48b6-ddbc-0618f6794c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/2qmgw9xk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e54b4215160>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import emoji\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import wandb\n",
        "from google.colab import drive, files\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e16a0e43",
      "metadata": {
        "id": "e16a0e43",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1e859a-d96f-4351-f68b-9da6a9e47d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/polar/dev_phase.zip\n",
            "   creating: subtask1/\n",
            "   creating: subtask1/dev/\n",
            "  inflating: subtask1/dev/nep.csv    \n",
            "  inflating: subtask1/dev/ita.csv    \n",
            "  inflating: subtask1/dev/pol.csv    \n",
            "  inflating: subtask1/dev/rus.csv    \n",
            "  inflating: subtask1/dev/tel.csv    \n",
            "  inflating: subtask1/dev/hin.csv    \n",
            "  inflating: subtask1/dev/hau.csv    \n",
            "  inflating: subtask1/dev/pan.csv    \n",
            "  inflating: subtask1/dev/ori.csv    \n",
            "  inflating: subtask1/dev/spa.csv    \n",
            "  inflating: subtask1/dev/deu.csv    \n",
            "  inflating: subtask1/dev/fas.csv    \n",
            "  inflating: subtask1/dev/arb.csv    \n",
            "  inflating: subtask1/dev/ben.csv    \n",
            "  inflating: subtask1/dev/amh.csv    \n",
            "  inflating: subtask1/dev/khm.csv    \n",
            "  inflating: subtask1/dev/tur.csv    \n",
            "  inflating: subtask1/dev/zho.csv    \n",
            "  inflating: subtask1/dev/eng.csv    \n",
            "  inflating: subtask1/dev/swa.csv    \n",
            "  inflating: subtask1/dev/urd.csv    \n",
            "  inflating: subtask1/dev/mya.csv    \n",
            "   creating: subtask1/train/\n",
            "  inflating: subtask1/train/nep.csv  \n",
            "  inflating: subtask1/train/pol.csv  \n",
            "  inflating: subtask1/train/rus.csv  \n",
            "  inflating: subtask1/train/ita.csv  \n",
            "  inflating: subtask1/train/hin.csv  \n",
            "  inflating: subtask1/train/tel.csv  \n",
            "  inflating: subtask1/train/fas.csv  \n",
            "  inflating: subtask1/train/deu.csv  \n",
            "  inflating: subtask1/train/hau.csv  \n",
            "  inflating: subtask1/train/pan.csv  \n",
            "  inflating: subtask1/train/ori.csv  \n",
            "  inflating: subtask1/train/spa.csv  \n",
            "  inflating: subtask1/train/arb.csv  \n",
            "  inflating: subtask1/train/khm.csv  \n",
            "  inflating: subtask1/train/tur.csv  \n",
            "  inflating: subtask1/train/zho.csv  \n",
            "  inflating: subtask1/train/amh.csv  \n",
            "  inflating: subtask1/train/ben.csv  \n",
            "  inflating: subtask1/train/swa.csv  \n",
            "  inflating: subtask1/train/urd.csv  \n",
            "  inflating: subtask1/train/eng.csv  \n",
            "  inflating: subtask1/train/mya.csv  \n",
            "   creating: subtask2/\n",
            "   creating: subtask2/train/\n",
            "  inflating: subtask2/train/nep.csv  \n",
            "  inflating: subtask2/train/ita.csv  \n",
            "  inflating: subtask2/train/rus.csv  \n",
            "  inflating: subtask2/train/pol.csv  \n",
            "  inflating: subtask2/train/hin.csv  \n",
            "  inflating: subtask2/train/tel.csv  \n",
            "  inflating: subtask2/train/deu.csv  \n",
            "  inflating: subtask2/train/fas.csv  \n",
            "  inflating: subtask2/train/pan.csv  \n",
            "  inflating: subtask2/train/hau.csv  \n",
            "  inflating: subtask2/train/spa.csv  \n",
            "  inflating: subtask2/train/ori.csv  \n",
            "  inflating: subtask2/train/arb.csv  \n",
            "  inflating: subtask2/train/amh.csv  \n",
            "  inflating: subtask2/train/zho.csv  \n",
            "  inflating: subtask2/train/tur.csv  \n",
            "  inflating: subtask2/train/khm.csv  \n",
            "  inflating: subtask2/train/ben.csv  \n",
            "  inflating: subtask2/train/swa.csv  \n",
            "  inflating: subtask2/train/urd.csv  \n",
            "  inflating: subtask2/train/eng.csv  \n",
            "  inflating: subtask2/train/mya.csv  \n",
            "   creating: subtask2/dev/\n",
            "  inflating: subtask2/dev/pol.csv    \n",
            "  inflating: subtask2/dev/rus.csv    \n",
            "  inflating: subtask2/dev/ita.csv    \n",
            "  inflating: subtask2/dev/nep.csv    \n",
            "  inflating: subtask2/dev/fas.csv    \n",
            "  inflating: subtask2/dev/deu.csv    \n",
            "  inflating: subtask2/dev/spa.csv    \n",
            "  inflating: subtask2/dev/ori.csv    \n",
            "  inflating: subtask2/dev/pan.csv    \n",
            "  inflating: subtask2/dev/hau.csv    \n",
            "  inflating: subtask2/dev/hin.csv    \n",
            "  inflating: subtask2/dev/tel.csv    \n",
            "  inflating: subtask2/dev/tur.csv    \n",
            "  inflating: subtask2/dev/zho.csv    \n",
            "  inflating: subtask2/dev/khm.csv    \n",
            "  inflating: subtask2/dev/amh.csv    \n",
            "  inflating: subtask2/dev/ben.csv    \n",
            "  inflating: subtask2/dev/arb.csv    \n",
            "  inflating: subtask2/dev/mya.csv    \n",
            "  inflating: subtask2/dev/urd.csv    \n",
            "  inflating: subtask2/dev/swa.csv    \n",
            "  inflating: subtask2/dev/eng.csv    \n",
            "   creating: subtask3/\n",
            "   creating: subtask3/dev/\n",
            "  inflating: subtask3/dev/eng.csv    \n",
            "  inflating: subtask3/dev/urd.csv    \n",
            "  inflating: subtask3/dev/swa.csv    \n",
            "  inflating: subtask3/dev/arb.csv    \n",
            "  inflating: subtask3/dev/ben.csv    \n",
            "  inflating: subtask3/dev/amh.csv    \n",
            "  inflating: subtask3/dev/tur.csv    \n",
            "  inflating: subtask3/dev/zho.csv    \n",
            "  inflating: subtask3/dev/khm.csv    \n",
            "  inflating: subtask3/dev/tel.csv    \n",
            "  inflating: subtask3/dev/hin.csv    \n",
            "  inflating: subtask3/dev/spa.csv    \n",
            "  inflating: subtask3/dev/ori.csv    \n",
            "  inflating: subtask3/dev/pan.csv    \n",
            "  inflating: subtask3/dev/hau.csv    \n",
            "  inflating: subtask3/dev/deu.csv    \n",
            "  inflating: subtask3/dev/fas.csv    \n",
            "  inflating: subtask3/dev/nep.csv    \n",
            "   creating: subtask3/train/\n",
            "  inflating: subtask3/train/amh.csv  \n",
            "  inflating: subtask3/train/zho.csv  \n",
            "  inflating: subtask3/train/tur.csv  \n",
            "  inflating: subtask3/train/khm.csv  \n",
            "  inflating: subtask3/train/ben.csv  \n",
            "  inflating: subtask3/train/arb.csv  \n",
            "  inflating: subtask3/train/swa.csv  \n",
            "  inflating: subtask3/train/urd.csv  \n",
            "  inflating: subtask3/train/eng.csv  \n",
            "  inflating: subtask3/train/nep.csv  \n",
            "  inflating: subtask3/train/deu.csv  \n",
            "  inflating: subtask3/train/fas.csv  \n",
            "  inflating: subtask3/train/pan.csv  \n",
            "  inflating: subtask3/train/hau.csv  \n",
            "  inflating: subtask3/train/spa.csv  \n",
            "  inflating: subtask3/train/ori.csv  \n",
            "  inflating: subtask3/train/hin.csv  \n",
            "  inflating: subtask3/train/tel.csv  \n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/polar/dev_phase.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocessing-section",
      "metadata": {
        "id": "preprocessing-section"
      },
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing-code",
      "metadata": {
        "id": "preprocessing-code"
      },
      "outputs": [],
      "source": [
        "def preprocess(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocessing function for social media text in both English and Hausa.\n",
        "    Preserves social media signals while reducing noise.\n",
        "    \"\"\"\n",
        "    # Replace usernames with @USER token\n",
        "    text = re.sub(r\"@[a-zA-Z0-9_]+\", \"@USER\", text)\n",
        "\n",
        "    # Replace URLs with HTTPURL token\n",
        "    text = re.sub(r\"https?://\\S+\", \"HTTPURL\", text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data-loading-section",
      "metadata": {
        "id": "data-loading-section"
      },
      "source": [
        "## Data Loading and Preparation\n",
        "\n",
        "Loading data for both English and Hausa languages for Subtask 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data-loading-code",
      "metadata": {
        "id": "data-loading-code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fccbd9d-629f-4207-e256-92a4873622af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English train samples: 3222\n",
            "Hausa train samples: 3651\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "eng_train = pd.read_csv('subtask1/train/eng.csv')\n",
        "hau_train = pd.read_csv('subtask1/train/hau.csv')\n",
        "\n",
        "print(f\"English train samples: {len(eng_train)}\")\n",
        "print(f\"Hausa train samples: {len(hau_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load augmented data\n",
        "# output_dir = \"/content/drive/MyDrive/polar/augmented_data_v1\"\n",
        "# eng_train = pd.read_csv(os.path.join(output_dir, \"english_augmented_train.csv\"))\n",
        "# hau_train = pd.read_csv(os.path.join(output_dir, \"hausa_augmented_train.csv\"))\n",
        "# print(f\"English augmented train samples: {len(eng_train)}\")\n",
        "# print(f\"Hausa augmented train samples: {len(hau_train)}\")"
      ],
      "metadata": {
        "id": "wP5l3yQj670d"
      },
      "id": "wP5l3yQj670d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back_translate = pd.read_csv(\"/content/drive/MyDrive/polar/back_translate_dataset_checkpoint.csv\")\n",
        "llm_paraphrase = pd.read_csv(\"/content/drive/MyDrive/polar/llm_augmented_data.csv\")\n",
        "full_data = pd.concat([back_translate, llm_paraphrase], ignore_index=True)"
      ],
      "metadata": {
        "id": "t5IakQ7QWrDL"
      },
      "id": "t5IakQ7QWrDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train = full_data[full_data['language'] == 'eng'].copy()\n",
        "hau_train = full_data[full_data['language'] == 'hau'].copy()\n",
        "\n",
        "print(f\"English augmented train samples: {len(eng_train)}\")\n",
        "print(f\"Hausa augmented train samples: {len(hau_train)}\")"
      ],
      "metadata": {
        "id": "MwoY42wCfd41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f365d407-b363-4e1e-ddde-361745996d88"
      },
      "id": "MwoY42wCfd41",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English augmented train samples: 5441\n",
            "Hausa augmented train samples: 7649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dev = pd.read_csv('subtask1/dev/eng.csv')\n",
        "hau_dev = pd.read_csv('subtask1/dev/hau.csv')\n",
        "print(f\"English dev samples: {len(eng_dev)}\")\n",
        "print(f\"Hausa dev samples: {len(hau_dev)}\")"
      ],
      "metadata": {
        "id": "fDv3gSB57BZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3007a24-8241-4418-f5d3-171ea697d614"
      },
      "id": "fDv3gSB57BZL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English dev samples: 160\n",
            "Hausa dev samples: 182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train.drop_duplicates(inplace=True)\n",
        "hau_train.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "kZNVLg-QRHrL"
      },
      "id": "kZNVLg-QRHrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train.shape, hau_train.shape"
      ],
      "metadata": {
        "id": "WcRwmTS3sc8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274ce824-8c51-459a-ae79-50e85bd40e79"
      },
      "id": "WcRwmTS3sc8u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5441, 4), (7649, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessing-application",
      "metadata": {
        "id": "preprocessing-application"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to all datasets\n",
        "eng_train['text'] = eng_train['text'].apply(preprocess)\n",
        "eng_dev['text'] = eng_dev['text'].apply(preprocess)\n",
        "\n",
        "hau_train['text'] = hau_train['text'].apply(preprocess)\n",
        "hau_dev['text'] = hau_dev['text'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-section",
      "metadata": {
        "id": "dataset-section"
      },
      "source": [
        "## Dataset Class\n",
        "\n",
        "Creating a PyTorch dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset-class",
      "metadata": {
        "id": "dataset-class"
      },
      "outputs": [],
      "source": [
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False,\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_tensors='pt')\n",
        "\n",
        "        # Remove .squeeze() to ensure consistent tensor shapes for DataCollatorWithPadding\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # item = {key: val for key, val in encoding.items()}\n",
        "\n",
        "        # Handle NaN labels gracefully for prediction sets (dev set)\n",
        "        if not (isinstance(label, (int, float)) and math.isnan(label)):\n",
        "            item['labels'] = torch.tensor(int(label), dtype=torch.long)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-section",
      "metadata": {
        "id": "model-section"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           num_labels=2\n",
        "                                                           )"
      ],
      "metadata": {
        "id": "tB687o6dyBAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "b010f42a0ded455cac6eadaea335950e",
            "d5439313bb624917aba6721b533ee618",
            "e1620306b3944d86b03fae64653780e8",
            "d316a21bd39d44f9b37444a64ba28732",
            "02e7cb047dc74117a84def9602962e76",
            "b1f32076955a49ed8bb81a689bd318b3",
            "5ad099d97e4e4d458e7123c2f8938b85",
            "6da78ae3d808410999ed534816653d11",
            "bd19c38fa864496ab5146cfbc36a15d1",
            "b4719033a9604646b3b64434c002cdc8",
            "03783b0611b64bb78c0046731a0988bf",
            "3e7481accddc41c288d3db17a236253d",
            "efd605c735044364a1e6c8948e2fe8a7",
            "a681b405bdda4c8fa8df458300a5bb33",
            "851a6976d933476883bdf8bee69edc27",
            "0e59f053cff846fda165ca48b3b2ec51",
            "05a0b9a7b92348cebe202c6023bf292f",
            "ef145c90d8254731a9e1333c3f8e7750",
            "13b66da517a24776b77869de7f9a977c",
            "bbb202f2ffcf46b5b2513f7bd1551a33",
            "f85b4167db2548e9812025efaecf6f7e",
            "894d0f08375245718123f75688e851f4",
            "b37a43ec89e64f708e208e0810e7c087",
            "6c5d051e8d4f44769ddae91cd87ab533",
            "b41c339e99dc4368a113411ab06921bc",
            "043b4a7ed4ec43b786e9b78eed62f5e1",
            "20ab0079814b4d5fb58d56c8cdd13c1e",
            "cd850f82d86d4cf695f437ccc9ec5eba",
            "0aec14054c1b44069e97e6ccf9aa7698",
            "fa923e90a5a44b7789d8d27e5aa92370",
            "fb7720a9a9e74f0eb54793722c2a6f0d",
            "f1120a1d0325434da63608566d131416",
            "bdf4ce23b3e34047be77c25b6f46b641",
            "1eb023ee51e2418d9b034a92626cd4c4",
            "cd1db27720ff4f759b29decfc3382591",
            "a097219bbfd047e9bdf73c833819080a",
            "91a38cdcb47d4bb29bd9e28c90298d1c",
            "ee46777b90d445a78b39b7adb5510832",
            "d8a8743b02c84269b38d7ac34ee8d91b",
            "182c4505fc334ffcb0c146bf0fd775fd",
            "9559449b53b2445aafa7b2e5876c2c87",
            "13046cf8f4434cc7a6132edf7448b551",
            "bd8b6f5f20d64da58a6eb8d17f25b00a",
            "be26631554dd4ec7b521eb97de000db8",
            "4d89446f0d174ae4b03584a9d77f858c",
            "f93e77a29ece439cb448bf3dfc163c23",
            "b68a766b90fe42a28928267ec42f79dd",
            "ac87bb0a0ffc45148719e3ecacdef1cb",
            "cc9d77306dcb4a478b9a055fc43941cb",
            "5694e063944b4973bb055582401019bf",
            "57c7ba3a8c81487382ea585ca7b434bc",
            "94f79b46e45344f4874af3874e140c83",
            "8059f7472c4a47d5838506aefd920aea",
            "3fc6700a9bb844119018ed96427eabd9",
            "27e3d4a08f1242e0b854ee7910099580",
            "55d28573c4bc4790a4302ea04e039163",
            "1dc3113e82e74f3fa2f3dffca27d41e3",
            "85774f30196b495899f7115f4fdc8155",
            "5083b3c5bfc84f459381b4d6aba346fd",
            "f33470eb423b488f8c1f2965f557d118",
            "25b9b5c7e874410286a4d487c15c3fd0",
            "f288799196bd4effa00e16ee4e7feffd",
            "794aa4259e304e90aefffa7761b7fe78",
            "b1628470e25448e09f17ed72e374771b",
            "727aae7705a944639031260aca12f8e1",
            "6079cda4b50144a78f56c4a894b02ea0"
          ]
        },
        "outputId": "c204c928-8e96-4c41-f95e-2fe48edaaea4"
      },
      "id": "tB687o6dyBAB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b010f42a0ded455cac6eadaea335950e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e7481accddc41c288d3db17a236253d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b37a43ec89e64f708e208e0810e7c087"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eb023ee51e2418d9b034a92626cd4c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/916M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d89446f0d174ae4b03584a9d77f858c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/916M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55d28573c4bc4790a4302ea04e039163"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at castorini/afriteva_base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 884,736 || all params: 230,505,218 || trainable%: 0.3838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRA configuration"
      ],
      "metadata": {
        "id": "ggBZOCUnPUq6"
      },
      "id": "ggBZOCUnPUq6"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRA attention dimension\n",
        "    lora_alpha=16,  # Alpha parameter for LoRA scaling\n",
        "    target_modules=[\"q\", \"v\"], # Modules to apply LoRA to\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",  # Bias type for LoRA layers\n",
        "    task_type=\"SEQ_CLS\"  # Task type\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "YnfbQ0GtPV2F"
      },
      "id": "YnfbQ0GtPV2F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "training-section",
      "metadata": {
        "id": "training-section"
      },
      "source": [
        "## Training Setup\n",
        "\n",
        "Preparing the data for training with both English and Hausa combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training-setup",
      "metadata": {
        "id": "training-setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a05aeb9-91d6-439e-be1a-76d2e1164834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined training samples: 13090\n",
            "Combined dev samples: 342\n"
          ]
        }
      ],
      "source": [
        "# Combine English and Hausa training data\n",
        "combined_train = pd.concat([eng_train, hau_train], ignore_index=True)\n",
        "combined_dev = pd.concat([eng_dev, hau_dev], ignore_index=True)\n",
        "\n",
        "print(f\"Combined training samples: {len(combined_train)}\")\n",
        "print(f\"Combined dev samples: {len(combined_dev)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split combined training data for validation\n",
        "train_data, val_data = train_test_split(\n",
        "    combined_train,\n",
        "    test_size=0.2,\n",
        "    stratify=combined_train['polarization'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = PolarizationDataset(\n",
        "    train_data['text'].tolist(),\n",
        "    train_data['polarization'].tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "val_dataset = PolarizationDataset(\n",
        "    val_data['text'].tolist(),\n",
        "    val_data['polarization'].tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "dev_dataset = PolarizationDataset(\n",
        "    combined_dev['text'].tolist(),\n",
        "    combined_dev['polarization'].tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "print(\"Datasets created successfully!\")"
      ],
      "metadata": {
        "id": "IjCoSwpX4NDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9136d89-2303-4944-908e-35a6568a620b"
      },
      "id": "IjCoSwpX4NDf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metrics-function",
      "metadata": {
        "id": "metrics-function"
      },
      "outputs": [],
      "source": [
        "# Define metrics function\n",
        "def compute_metrics(p):\n",
        "    # Ensure p.predictions is the actual logits tensor/array\n",
        "    if isinstance(p.predictions, tuple):\n",
        "        logits = p.predictions[0]\n",
        "    else:\n",
        "        logits = p.predictions\n",
        "\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    weight_decay=0.01,\n",
        "    label_smoothing_factor=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "trainer-section",
      "metadata": {
        "id": "trainer-section"
      },
      "source": [
        "## Training\n",
        "\n",
        "Initialize the trainer and start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trainer-initialization",
      "metadata": {
        "id": "trainer-initialization",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f1389938-6860-4892-fbcf-dab4d6fe189c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer initialized successfully with LoRA-wrapped model!\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully with LoRA-wrapped model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training-execution",
      "metadata": {
        "id": "training-execution",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "f829b78e-689a-478b-f60d-82dedc69a90d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6291' max='6545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6291/6545 28:11 < 01:08, 3.72 it/s, Epoch 4.81/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.467400</td>\n",
              "      <td>0.516352</td>\n",
              "      <td>0.784434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.471500</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.825277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.422700</td>\n",
              "      <td>0.436953</td>\n",
              "      <td>0.841438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.428471</td>\n",
              "      <td>0.850787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6545' max='6545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6545/6545 29:56, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.467400</td>\n",
              "      <td>0.516352</td>\n",
              "      <td>0.784434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.471500</td>\n",
              "      <td>0.446972</td>\n",
              "      <td>0.825277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.422700</td>\n",
              "      <td>0.436953</td>\n",
              "      <td>0.841438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.428471</td>\n",
              "      <td>0.850787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.433000</td>\n",
              "      <td>0.427169</td>\n",
              "      <td>0.854869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='328' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [328/328 00:40]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Macro F1 score on validation set: 0.8548690730080336\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set: {eval_results['eval_f1_macro']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "predictions-section",
      "metadata": {
        "id": "predictions-section"
      },
      "source": [
        "## Generate Predictions\n",
        "\n",
        "Generate predictions for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "language-evaluation",
      "metadata": {
        "id": "language-evaluation"
      },
      "outputs": [],
      "source": [
        "eng_dev_dataset = PolarizationDataset(\n",
        "    eng_dev['text'].tolist(),\n",
        "    eng_dev['polarization'].tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "hau_dev_dataset = PolarizationDataset(\n",
        "    hau_dev['text'].tolist(),\n",
        "    hau_dev['polarization'].tolist(),\n",
        "    tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vChBQkDj3_5m",
      "metadata": {
        "id": "vChBQkDj3_5m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "bfbbbd47-baf8-41de-ea35-6507cc3fbc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for English...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English predictions saved to subtask_1/pred_eng.csv\n",
            "                                     id  polarization\n",
            "0  eng_f66ca14d60851371f9720aaf4ccd9b58             0\n",
            "1  eng_3a489aa7fed9726aa8d3d4fe74c57efb             0\n",
            "2  eng_95770ff547ea5e48b0be00f385986483             0\n",
            "3  eng_2048ae6f9aa261c48e6d777bcc5b38bf             0\n",
            "4  eng_07781aa88e61e7c0a996abd1e5ea3a20             0\n",
            "Generating predictions for Hausa...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hausa predictions saved to subtask_1/pred_hau.csv\n",
            "                                     id  polarization\n",
            "0  hau_7bafacd606d9dee74e7cee95f8277d4e             0\n",
            "1  hau_cbf1bdd94361d60e55c6774b2a69198a             0\n",
            "2  hau_8e2286abdaa2b53c5a43e2a13e11cddd             0\n",
            "3  hau_1fcff27ecdc63dca7852720481daf56d             1\n",
            "4  hau_fbfe2c9ca8b5bb50f1fd4cd295c95a15             0\n"
          ]
        }
      ],
      "source": [
        "output_dir_lang = 'subtask_1'\n",
        "os.makedirs(output_dir_lang, exist_ok=True)\n",
        "\n",
        "# --- Generate predictions for English ---\n",
        "print(\"Generating predictions for English...\")\n",
        "predictions_eng = trainer.predict(eng_dev_dataset)\n",
        "predicted_labels_eng = np.argmax(predictions_eng.predictions[0], axis=1)\n",
        "ids_eng = eng_dev['id'].tolist()\n",
        "\n",
        "submission_df_eng = pd.DataFrame({\n",
        "    'id': ids_eng,\n",
        "    'polarization': predicted_labels_eng\n",
        "})\n",
        "\n",
        "output_filename_eng = os.path.join(output_dir_lang, 'pred_eng.csv')\n",
        "submission_df_eng.to_csv(output_filename_eng, index=False)\n",
        "\n",
        "print(f\"English predictions saved to {output_filename_eng}\")\n",
        "print(submission_df_eng.head())\n",
        "\n",
        "# --- Generate predictions for Hausa ---\n",
        "print(\"Generating predictions for Hausa...\")\n",
        "predictions_hau = trainer.predict(hau_dev_dataset)\n",
        "predicted_labels_hau = np.argmax(predictions_hau.predictions[0], axis=1)\n",
        "ids_hau = hau_dev['id'].tolist()\n",
        "\n",
        "submission_df_hau = pd.DataFrame({\n",
        "    'id': ids_hau,\n",
        "    'polarization': predicted_labels_hau\n",
        "})\n",
        "\n",
        "output_filename_hau = os.path.join(output_dir_lang, 'pred_hau.csv')\n",
        "submission_df_hau.to_csv(output_filename_hau, index=False)\n",
        "\n",
        "print(f\"Hausa predictions saved to {output_filename_hau}\")\n",
        "print(submission_df_hau.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_filename = 'subtask_1.zip'\n",
        "\n",
        "# Create a temporary nested structure\n",
        "temp_parent = '/content/temp_parent'\n",
        "target_dir = os.path.join(temp_parent, 'subtask_1', 'subtask_1')\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Copy all CSV files into the nested directory\n",
        "for file in os.listdir(output_dir_lang):\n",
        "    if file.endswith('.csv'):\n",
        "        shutil.copy2(\n",
        "            os.path.join(output_dir_lang, file),\n",
        "            os.path.join(target_dir, file)\n",
        "        )\n",
        "\n",
        "# Zip the parent directory (which contains subtask_1/subtask_1/)\n",
        "shutil.make_archive('subtask_1', 'zip', temp_parent)\n",
        "\n",
        "# Clean up\n",
        "shutil.rmtree(temp_parent)\n",
        "\n",
        "print(f'Archive created: {zip_filename}')\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "hh0EFms0CAdA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0297a9ce-d891-4d13-8b3a-c491d9b8bc37"
      },
      "id": "hh0EFms0CAdA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive created: subtask_1.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_521445e2-f584-488c-8b83-4f91d1dd90ac\", \"subtask_1.zip\", 7490)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "next-steps",
      "metadata": {
        "id": "next-steps"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "1. Back-translation augmentation for Hausa data\n",
        "2. LLM paraphrasing\n",
        "3. EDA augmentation\n",
        "4. Hausa → English translation with silver labels\n",
        "5. Contextual word masking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jn13x5HaFwJj",
      "metadata": {
        "id": "Jn13x5HaFwJj"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JHOJWkAAIGo4",
      "metadata": {
        "id": "JHOJWkAAIGo4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] accelerate\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U2lO8a0jGkEE",
      "metadata": {
        "id": "U2lO8a0jGkEE"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxKmv25vG-Xv"
      },
      "source": [
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "id": "KxKmv25vG-Xv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7uMftWnUGFPw",
      "metadata": {
        "id": "7uMftWnUGFPw"
      },
      "source": [
        "## Back-translation Augmentation for Hausa Texts\n",
        "\n",
        "we'll implement back-translation using the facebook/nllb-200-3.3B-distilled model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5gCSumJKGsXb",
      "metadata": {
        "id": "5gCSumJKGsXb"
      },
      "outputs": [],
      "source": [
        "# Set up NLLB model for translation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "print(\"Loading NLLB translation models...\")\n",
        "\n",
        "try:\n",
        "    translator_eng = pipeline(\n",
        "        \"translation\",\n",
        "        model=\"facebook/nllb-200-distilled-1.3B\",\n",
        "        src_lang=\"hau_Latn\",\n",
        "        tgt_lang=\"eng_Latn\",\n",
        "        device=device,\n",
        "        dtype=dtype,\n",
        "        batch_size=16\n",
        "    )\n",
        "\n",
        "    translator_back = pipeline(\n",
        "        \"translation\",\n",
        "        model=\"facebook/nllb-200-distilled-1.3B\",\n",
        "        src_lang=\"eng_Latn\",\n",
        "        tgt_lang=\"hau_Latn\",\n",
        "        device=device,\n",
        "        dtype=dtype,\n",
        "        batch_size=16\n",
        "    )\n",
        "\n",
        "    print(\"Translation models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading translation models: {e}\")\n",
        "    translator_eng = None\n",
        "    translator_back = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w4ZiR-qDMX6X",
      "metadata": {
        "id": "w4ZiR-qDMX6X"
      },
      "outputs": [],
      "source": [
        "def backtranslate_hausa(text, similarity_threshold=0.75):\n",
        "    \"\"\"\n",
        "    Perform back-translation augmentation for Hausa text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Original Hausa text\n",
        "        similarity_threshold (float): Minimum similarity to accept augmented text\n",
        "\n",
        "    Returns:\n",
        "        str: Back-translated text if acceptable, else original text\n",
        "    \"\"\"\n",
        "    if translator_eng is None or translator_back is None:\n",
        "        return text\n",
        "\n",
        "    try:\n",
        "        # Translate Hausa to English\n",
        "        eng_translation = translator_eng(text, max_length=400)[0]['translation_text']\n",
        "\n",
        "        # Translate back to Hausa\n",
        "        back_translation = translator_back(eng_translation, max_length=400)[0]['translation_text']\n",
        "\n",
        "        # For now, we'll use a basic string similarity check\n",
        "        if len(text) > 0 and len(back_translation) > 0:\n",
        "            # If the back-translated text is too different, reject it\n",
        "            # This is a simplified check - in practice use instructor-xl\n",
        "            if abs(len(back_translation) - len(text)) / len(text) < 0.5:\n",
        "                return back_translation\n",
        "            else:\n",
        "                return text\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Back-translation failed for text: {text[:50]}... Error: {e}\")\n",
        "        return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OfWup-3ZMZKZ",
      "metadata": {
        "id": "OfWup-3ZMZKZ"
      },
      "outputs": [],
      "source": [
        "def augment_polarized_hausa(data, num_augmentations=3):\n",
        "    \"\"\"\n",
        "    Augment polarized Hausa examples using batched back-translation.\n",
        "    Significantly faster than row-by-row processing.\n",
        "    \"\"\"\n",
        "    # 1. Filter for polarized Hausa examples\n",
        "    polarized_hausa = data[(data['polarization'] == 1) & (data['language'] == 'hau')].copy()\n",
        "\n",
        "    if len(polarized_hausa) == 0:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(polarized_hausa)} polarized Hausa examples for augmentation.\")\n",
        "\n",
        "    # Extract lists for batch processing\n",
        "    original_texts = polarized_hausa['text'].tolist()\n",
        "    original_ids = polarized_hausa['id'].tolist()\n",
        "\n",
        "    augmented_ids = []\n",
        "    augmented_texts = []\n",
        "    augmented_labels = []\n",
        "    augmented_languages = []\n",
        "\n",
        "    # 2. Perform augmentation loops\n",
        "    # We loop 'num_augmentations' times, processing the whole dataset in batches each time\n",
        "    for i in range(num_augmentations):\n",
        "        print(f\"Processing augmentation round {i+1}/{num_augmentations}...\")\n",
        "\n",
        "        # --- Step A: Batch Translate Hausa -> English ---\n",
        "        # We use standard generation (deterministic) for the forward pass\n",
        "        # The pipeline handles the batching internally via the batch_size param set earlier\n",
        "        print(\"  Translating Hausa -> English...\")\n",
        "        eng_results = translator_eng(original_texts, max_length=400)\n",
        "        eng_texts = [res['translation_text'] for res in eng_results]\n",
        "\n",
        "        # --- Step B: Batch Translate English -> Hausa ---\n",
        "        # We enable sampling (do_sample=True) here to introduce variety in the back-translation\n",
        "        # temperature controls creativity (0.7 is usually a good balance)\n",
        "        print(\"  Translating English -> Hausa...\")\n",
        "        hau_results = translator_back(\n",
        "            eng_texts,\n",
        "            max_length=400,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_k=50\n",
        "        )\n",
        "        back_translated_texts = [res['translation_text'] for res in hau_results]\n",
        "\n",
        "        # --- Step C: Validation and Collection ---\n",
        "        print(\"  Validating and collecting results...\")\n",
        "        for orig_text, new_text, orig_id in zip(original_texts, back_translated_texts, original_ids):\n",
        "\n",
        "            # Simple validation: Text shouldn't be identical, and length shouldn't be drastically different\n",
        "            if orig_text != new_text:\n",
        "                len_ratio = len(new_text) / len(orig_text) if len(orig_text) > 0 else 0\n",
        "\n",
        "                # Keep if length is within 50% - 150% of original\n",
        "                if 0.5 < len_ratio < 1.5:\n",
        "                    augmented_ids.append(f\"{orig_id}_aug_bt_{i}\")\n",
        "                    augmented_texts.append(new_text)\n",
        "                    augmented_labels.append(1) # Label is preserved\n",
        "                    augmented_languages.append('hau')\n",
        "\n",
        "    # Create DataFrame\n",
        "    augmented_df = pd.DataFrame({\n",
        "        'id': augmented_ids,\n",
        "        'text': augmented_texts,\n",
        "        'polarization': augmented_labels,\n",
        "        'language': augmented_languages\n",
        "    })\n",
        "\n",
        "    print(f\"Batch processing complete. Generated {len(augmented_df)} augmented examples.\")\n",
        "    return augmented_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pEkDZCDPGLlC",
      "metadata": {
        "id": "pEkDZCDPGLlC"
      },
      "source": [
        "### Testing Back-translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4sCgQkxGyHg",
      "metadata": {
        "id": "f4sCgQkxGyHg"
      },
      "outputs": [],
      "source": [
        "# Test back-translation on sample texts\n",
        "sample_texts = [\n",
        "    \"Ƙungiyar tarayya ce ta zamana kansa a Nigeria.\",\n",
        "    \"Ana bukatar gwamnati don taimaka mutane.\",\n",
        "    \"Ƙungiyoyin da suka faru a shekarun 2000.\",\n",
        "    \"Ƙasar Nigeria tana da al'umma mai yawa.\"\n",
        "]\n",
        "\n",
        "print(\"Testing back-translation function:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Original:    {text}\")\n",
        "\n",
        "    # Apply back-translation\n",
        "    augmented = backtranslate_hausa(text)\n",
        "    print(f\"Augmented:   {augmented}\")\n",
        "\n",
        "    # Show if it changed\n",
        "    if text != augmented:\n",
        "        print(\"Status:      AUGMENTED\")\n",
        "    else:\n",
        "        print(\"Status:      UNCHANGED\")\n",
        "\n",
        "# Test on actual Hausa data\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Testing on actual Hausa training data:\")\n",
        "\n",
        "# Get a few polarized Hausa examples from our training data\n",
        "if 'hau_train' in locals():\n",
        "    polarized_hau_samples = hau_train[hau_train['polarization'] == 1].head(3)\n",
        "    print(f\"Found {len(polarized_hau_samples)} polarized Hausa samples\")\n",
        "\n",
        "    for idx, row in polarized_hau_samples.iterrows():\n",
        "        print(f\"\\nOriginal ID: {row['id']}\")\n",
        "        print(f\"Original:    {row['text']}\")\n",
        "\n",
        "        # Apply back-translation\n",
        "        augmented = backtranslate_hausa(row['text'])\n",
        "        print(f\"Augmented:   {augmented}\")\n",
        "\n",
        "        # Show if it changed\n",
        "        if row['text'] != augmented:\n",
        "            print(\"Status:      AUGMENTED\")\n",
        "        else:\n",
        "            print(\"Status:      UNCHANGED\")\n",
        "else:\n",
        "    print(\"Hausa training data not available for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xeUOaPjrGPOz",
      "metadata": {
        "id": "xeUOaPjrGPOz"
      },
      "source": [
        "### Applying Augmentation to Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xIPM9ikIG1f8",
      "metadata": {
        "id": "xIPM9ikIG1f8"
      },
      "outputs": [],
      "source": [
        "# Prepare combined training data\n",
        "eng_train['language'] = 'eng'\n",
        "hau_train['language'] = 'hau'\n",
        "combined_train = pd.concat([eng_train, hau_train], ignore_index=True)\n",
        "\n",
        "# Apply BATCH augmentation\n",
        "print(\"\\nApplying batched back-translation augmentation...\")\n",
        "augmented_data = augment_polarized_hausa(combined_train, num_augmentations=3)\n",
        "\n",
        "# Combine and Save\n",
        "augmented_train = pd.concat([combined_train, augmented_data], ignore_index=True)\n",
        "\n",
        "print(f\"\\nFinal training data shape: {augmented_train.shape}\")\n",
        "print(augmented_train['polarization'].value_counts().sort_index())\n",
        "\n",
        "# Save checkpoints\n",
        "augmented_train.to_csv(\"/content/drive/MyDrive/polar/back_translate_dataset_checkpoint.csv\", index=False)\n",
        "augmented_data.to_csv(\"/content/drive/MyDrive/polar/augmented_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to load the checkpoint back ---\n",
        "\n",
        "try:\n",
        "    augmented_train = pd.read_csv(\"/content/drive/MyDrive/polar/back_translate_dataset_checkpoint.csv\")\n",
        "    augmented_data = pd.read_csv(\"/content/drive/MyDrive/polar/augmented_data.csv\")\n",
        "    print(f\"\\nSuccessfully loaded data from checkpoint\")\n",
        "    print(f\"Loaded dataset shape: {augmented_train.shape}\")\n",
        "    print(\"Loaded dataset head:\\n\", augmented_train.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: Checkpoint file not found. Please ensure it exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while loading the checkpoint: {e}\")"
      ],
      "metadata": {
        "id": "r-TkhfSxlwoH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "r-TkhfSxlwoH"
    },
    {
      "cell_type": "markdown",
      "id": "GyS2oF8zGRiY",
      "metadata": {
        "id": "GyS2oF8zGRiY"
      },
      "source": [
        "## LLM Paraphrasing Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c682d6",
      "metadata": {
        "id": "b0c682d6"
      },
      "outputs": [],
      "source": [
        "llm_paraphrase_pipeline = None\n",
        "\n",
        "print(\"Initializing multilingual LLM for paraphrasing...\")\n",
        "try:\n",
        "    llm_paraphrase_pipeline = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"google/mt5-large\",\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "        batch_size=8\n",
        "    )\n",
        "    print(\"LLM paraphrasing pipeline initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing LLM paraphrasing pipeline: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_paraphrase_via_infilling(text: str, language: str = 'eng') -> str:\n",
        "    \"\"\"\n",
        "    Paraphrases text by heavily masking it (35%) and letting mT5 regenerate the gaps.\n",
        "    \"\"\"\n",
        "    # 1. Mask a significant portion of the text (35%) to force rewriting\n",
        "    masked_version = mask_words_contextual(text, mask_ratio=0.35)\n",
        "\n",
        "    # 2. If masking failed (text too short), return original\n",
        "    if \"<extra_id_\" not in masked_version:\n",
        "        return text\n",
        "\n",
        "    # 3. Infill using the CORRECTED logic (with stitching)\n",
        "    try:\n",
        "        paraphrased = infill_masked_text(masked_version)\n",
        "        return paraphrased\n",
        "    except Exception as e:\n",
        "        return text"
      ],
      "metadata": {
        "id": "yNpPnal_4bXQ"
      },
      "id": "yNpPnal_4bXQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c34e0ed",
      "metadata": {
        "id": "1c34e0ed"
      },
      "outputs": [],
      "source": [
        "# def llm_paraphrase(text, language='english'):\n",
        "#     \"\"\"\n",
        "#     Paraphrase text using an LLM while preserving meaning and tone.\n",
        "\n",
        "#     Args:\n",
        "#         text (str): Original text to paraphrase\n",
        "#         language (str): Language of the text ('english' or 'hausa')\n",
        "\n",
        "#     Returns:\n",
        "#         str: Paraphrased text\n",
        "#     \"\"\"\n",
        "#     global llm_paraphrase_pipeline\n",
        "\n",
        "#     if llm_paraphrase_pipeline is None:\n",
        "#         print(\"LLM paraphrasing pipeline not available. Returning original text.\")\n",
        "#         return text\n",
        "\n",
        "#     try:\n",
        "#         input_text = f\"paraphrase: {text}\"\n",
        "\n",
        "#         paraphrased_output = llm_paraphrase_pipeline(input_text, max_new_tokens=128,\n",
        "#                                                      min_new_tokens=10, num_beams=4,\n",
        "#                                                      early_stopping=True,\n",
        "#                                                      no_repeat_ngram_size=2,\n",
        "#                                                      do_sample=True,\n",
        "#                                                      temperature=0.7, top_k=50,\n",
        "#                                                      top_p=0.95)\n",
        "#         paraphrased_text = paraphrased_output[0]['generated_text']\n",
        "\n",
        "#         # Ensure the paraphrased text is not identical to the original\n",
        "#         if paraphrased_text.strip() == text.strip():\n",
        "#             return text # Return original if no change\n",
        "\n",
        "#         return paraphrased_text\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error during LLM paraphrasing for text: '{text[:50]}...'. Error: {e}\")\n",
        "#         return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mirotQkXM5nM",
      "metadata": {
        "id": "mirotQkXM5nM"
      },
      "outputs": [],
      "source": [
        "# def augment_with_llm_paraphrasing(data, num_paraphrases=2):\n",
        "#     \"\"\"\n",
        "#     Augment polarized examples using LLM paraphrasing.\n",
        "\n",
        "#     Args:\n",
        "#         data (pd.DataFrame): DataFrame containing texts with polarization labels\n",
        "#         num_paraphrases (int): Number of paraphrases per example\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Augmented dataset\n",
        "#     \"\"\"\n",
        "#     # Filter for polarized examples (label = 1)\n",
        "#     polarized_data = data[data['polarization'] == 1].copy()\n",
        "\n",
        "#     print(f\"Found {len(polarized_data)} polarized examples for LLM paraphrasing\")\n",
        "\n",
        "#     augmented_texts = []\n",
        "#     augmented_ids = []\n",
        "#     augmented_labels = []\n",
        "#     augmented_languages = []\n",
        "\n",
        "#     # Perform paraphrasing for each polarized example\n",
        "#     for idx, row in polarized_data.iterrows():\n",
        "#         original_text = row['text']\n",
        "#         language = 'hausa' if row['language'] == 'hau' else 'english'\n",
        "\n",
        "#         # Generate multiple paraphrases\n",
        "#         for i in range(num_paraphrases):\n",
        "#             paraphrased_text = llm_paraphrase(original_text, language)\n",
        "\n",
        "#             # Only add if it's different from original\n",
        "#             if paraphrased_text != original_text:\n",
        "#                 augmented_texts.append(paraphrased_text)\n",
        "#                 augmented_ids.append(f\"{row['id']}_aug_llm_{i}\")\n",
        "#                 augmented_labels.append(row['polarization'])  # Keep same label\n",
        "#                 augmented_languages.append(row['language'])\n",
        "\n",
        "#     # Create DataFrame with augmented data\n",
        "#     augmented_df = pd.DataFrame({\n",
        "#         'id': augmented_ids,\n",
        "#         'text': augmented_texts,\n",
        "#         'polarization': augmented_labels,\n",
        "#         'language': augmented_languages\n",
        "#     })\n",
        "\n",
        "#     print(f\"Generated {len(augmented_df)} LLM paraphrased examples\")\n",
        "#     return augmented_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_words_contextual(text: str, mask_ratio: float = 0.35) -> str:\n",
        "    \"\"\"\n",
        "    Randomly masks spans of text with <extra_id_X> tokens.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < 5:\n",
        "        return text\n",
        "\n",
        "    n_words = len(words)\n",
        "    n_to_mask = max(1, int(n_words * mask_ratio))\n",
        "    indices = sorted(random.sample(range(n_words), n_to_mask))\n",
        "\n",
        "    spans = []\n",
        "    i = 0\n",
        "    while i < len(indices):\n",
        "        start = indices[i]\n",
        "        end = start\n",
        "        while (i + 1 < len(indices) and indices[i + 1] == indices[i] + 1 and end - start < 10):\n",
        "            i += 1\n",
        "            end = indices[i]\n",
        "        spans.append((start, end + 1))\n",
        "        i += 1\n",
        "\n",
        "    result = []\n",
        "    prev_end = 0\n",
        "    for idx, (start, end) in enumerate(spans):\n",
        "        result.extend(words[prev_end:start])\n",
        "        result.append(f\"<extra_id_{idx}>\")\n",
        "        prev_end = end\n",
        "    result.extend(words[prev_end:])\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "def stitch_t5_spans(masked_text, generated_text):\n",
        "    \"\"\"\n",
        "    Reconstructs the full text by combining the masked input\n",
        "    and the model's span outputs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Split input by <extra_id_X>\n",
        "        input_parts = re.split(r'<extra_id_\\d+>', masked_text)\n",
        "\n",
        "        # Split output by <extra_id_X> (and remove empty strings)\n",
        "        # T5 output usually starts with <extra_id_0> content <extra_id_1>...\n",
        "        gen_parts = re.split(r'<extra_id_\\d+>', generated_text)\n",
        "        gen_parts = [g.strip() for g in gen_parts if g.strip()]\n",
        "\n",
        "        result = \"\"\n",
        "        # Interleave parts\n",
        "        for i in range(len(input_parts)):\n",
        "            result += input_parts[i].strip() + \" \"\n",
        "            if i < len(gen_parts):\n",
        "                result += gen_parts[i].strip() + \" \"\n",
        "\n",
        "        return result.strip()\n",
        "    except Exception:\n",
        "        return masked_text # Fallback to original if stitching fails"
      ],
      "metadata": {
        "id": "LjER0-l65kcM"
      },
      "id": "LjER0-l65kcM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_with_llm_paraphrasing_batch(data: pd.DataFrame, num_paraphrases: int = 2) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Augment polarized examples using mT5 Infilling Paraphrasing in BATCHES.\n",
        "    \"\"\"\n",
        "    if llm_paraphrase_pipeline is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter for polarized examples\n",
        "    polarized_data = data[data['polarization'] == 1].copy()\n",
        "    print(f\"Preparing paraphrases for {len(polarized_data)} examples...\")\n",
        "\n",
        "    # Lists to hold batch data\n",
        "    batch_inputs = []\n",
        "    metadata = [] # Stores (original_id, original_text, language, iteration_index)\n",
        "\n",
        "    # 1. Pre-generate all masked inputs\n",
        "    for idx, row in polarized_data.iterrows():\n",
        "        text = row['text'].strip()\n",
        "\n",
        "        for i in range(num_paraphrases):\n",
        "            # Create masked version\n",
        "            masked_text = mask_words_contextual(text, mask_ratio=0.35)\n",
        "\n",
        "            # Only add if masking actually happened\n",
        "            if \"<extra_id_\" in masked_text:\n",
        "                batch_inputs.append(masked_text)\n",
        "                metadata.append({\n",
        "                    'orig_id': row['id'],\n",
        "                    'orig_text': text,\n",
        "                    'language': row.get('language', 'eng'),\n",
        "                    'iter': i,\n",
        "                    'masked_input': masked_text\n",
        "                })\n",
        "\n",
        "    print(f\"Running batch inference on {len(batch_inputs)} items...\")\n",
        "\n",
        "    # 2. Run Batch Inference\n",
        "    # We use num_beams=5 to get high quality generation for the gaps\n",
        "    outputs = llm_paraphrase_pipeline(\n",
        "        batch_inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=5\n",
        "    )\n",
        "\n",
        "    # 3. Process Results\n",
        "    new_rows = []\n",
        "\n",
        "    print(\"Stitching and validating results...\")\n",
        "    for meta, output in zip(metadata, outputs):\n",
        "        generated_spans = output['generated_text']\n",
        "\n",
        "        # Stitch back together\n",
        "        final_text = stitch_t5_spans(meta['masked_input'], generated_spans)\n",
        "\n",
        "        # Quality Checks\n",
        "        orig_text = meta['orig_text']\n",
        "\n",
        "        if (final_text != orig_text and                 # Must be different\n",
        "            len(final_text.split()) > 3 and             # Not too short\n",
        "            len(final_text) < len(orig_text) * 2 and    # Not hallucinating too much length\n",
        "            final_text.lower() not in orig_text.lower()): # Not just a substring\n",
        "\n",
        "            new_rows.append({\n",
        "                'id': f\"{meta['orig_id']}_para_{meta['iter']}_{random.randint(100,999)}\",\n",
        "                'text': final_text,\n",
        "                'polarization': 1,\n",
        "                'language': meta['language']\n",
        "            })\n",
        "\n",
        "    result_df = pd.DataFrame(new_rows)\n",
        "    print(f\"Generated {len(result_df)} valid paraphrases.\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "sYBoXUhl6DIa"
      },
      "id": "sYBoXUhl6DIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5VSZIQfDGTOK",
      "metadata": {
        "id": "5VSZIQfDGTOK"
      },
      "source": [
        "### Testing LLM Paraphrasing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- TEST THE FIX -------------------\n",
        "sample_texts = [\n",
        "    (\"This is a polarized English statement.\", \"english\"),\n",
        "    (\"Ƙungiyar tarayya ce ta zamana kansa.\", \"hausa\"),\n",
        "    (\"We must stand together against injustice.\", \"english\"),\n",
        "    (\"Ana bukatar gwamnati don taimaka mutane.\", \"hausa\")\n",
        "]\n",
        "\n",
        "print(\"Testing mT5 Infilling Paraphraser:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, (text, lang) in enumerate(sample_texts):\n",
        "    print(f\"\\nSample {i+1} ({lang}):\")\n",
        "    print(f\"Original:    {text}\")\n",
        "\n",
        "    # Apply Paraphrasing\n",
        "    paraphrased = llm_paraphrase_via_infilling(text, lang)\n",
        "    print(f\"Paraphrased: {paraphrased}\")\n",
        "\n",
        "    if text != paraphrased:\n",
        "        print(\"Status:      SUCCESS\")\n",
        "    else:\n",
        "        print(\"Status:      UNCHANGED (Text might be too short)\")"
      ],
      "metadata": {
        "id": "3zSnNwaGQVeq"
      },
      "id": "3zSnNwaGQVeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LLM paraphrasing to our previously augmented training data\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Applying Batch LLM paraphrasing...\")\n",
        "\n",
        "llm_augmented_data = augment_with_llm_paraphrasing_batch(augmented_train, num_paraphrases=2)\n",
        "\n",
        "print(f\"LLM paraphrased data shape: {llm_augmented_data.shape}\")\n",
        "\n",
        "# Combine with existing augmented data\n",
        "fully_augmented_train = pd.concat([augmented_train, llm_augmented_data], ignore_index=True)\n",
        "print(f\"Fully augmented training data shape: {fully_augmented_train.shape}\")\n",
        "\n",
        "# Show final statistics\n",
        "print(\"\\nFinal Data Statistics:\")\n",
        "print(f\"Total samples: {len(fully_augmented_train)}\")\n",
        "print(\"\\nFinal Polarization Distribution:\")\n",
        "print(fully_augmented_train['polarization'].value_counts().sort_index())\n",
        "\n",
        "# --- Save the complete augmented dataset ---\n",
        "checkpoint_filename = \"/content/drive/MyDrive/polar/paraphrase_dataset_checkpoint.csv\"\n",
        "fully_augmented_train.to_csv(checkpoint_filename, index=False)\n",
        "llm_augmented_data.to_csv(\"/content/drive/MyDrive/polar/llm_augmented_data.csv\", index=False)\n",
        "print(f\"Data checkpoint saved to: {checkpoint_filename}\")"
      ],
      "metadata": {
        "id": "usT1yNB6WAe9"
      },
      "id": "usT1yNB6WAe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to load the checkpoint back (for later use or if notebook restarts) ---\n",
        "\n",
        "try:\n",
        "    fully_augmented_train = pd.read_csv(\"/content/drive/MyDrive/polar/paraphrase_dataset_checkpoint.csv\")\n",
        "    llm_augmented_data = pd.read_csv(\"/content/drive/MyDrive/polar/llm_augmented_data.csv\")\n",
        "    print(f\"\\nSuccessfully loaded data from checkpoint\")\n",
        "    print(f\"Loaded dataset shape: {fully_augmented_train.shape}\")\n",
        "    print(\"Loaded dataset head:\\n\", fully_augmented_train.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: Checkpoint file not found. Please ensure it exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while loading the checkpoint: {e}\")"
      ],
      "metadata": {
        "id": "-VFDI1gOlo2D"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-VFDI1gOlo2D"
    },
    {
      "cell_type": "markdown",
      "id": "Ds2F4DodGUvm",
      "metadata": {
        "id": "Ds2F4DodGUvm"
      },
      "source": [
        "## EDA (Easy Data Augmentation) Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Initializing EDA models...\")\n",
        "\n",
        "# Multilingual contextual\n",
        "contextual_aug = naw.ContextualWordEmbsAug(\n",
        "    model_path=\"FacebookAI/xlm-roberta-large\",\n",
        "    action=\"substitute\",\n",
        "    aug_p=0.3,\n",
        "    top_k=10,\n",
        "    device=device,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "insert_aug = naw.ContextualWordEmbsAug(\n",
        "    model_path=\"FacebookAI/xlm-roberta-large\",\n",
        "    action=\"insert\",\n",
        "    aug_p=0.25,\n",
        "    device=device,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "delete_aug = naw.RandomWordAug(action=\"delete\", aug_p=0.2)\n",
        "swap_aug   = naw.RandomWordAug(action=\"swap\", aug_max=4)\n",
        "english_syn_aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n",
        "\n",
        "print(\"EDA models initialized.\")"
      ],
      "metadata": {
        "id": "e01RYl08czq5"
      },
      "id": "e01RYl08czq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== INDIVIDUAL EDA FUNCTIONS ======================\n",
        "def eda_synonym_replacement(text: str, language: str = 'english'):\n",
        "    if language == 'english':\n",
        "        return english_syn_aug.augment(text)[0] if text.strip() else text\n",
        "    else:\n",
        "        return contextual_aug.augment(text)[0] if text.strip() else text\n",
        "\n",
        "def eda_random_insertion(text: str, language: str = 'english'):\n",
        "    return insert_aug.augment(text)[0] if text.strip() else text\n",
        "\n",
        "def eda_random_deletion(text: str, language: str = 'english'):\n",
        "    return delete_aug.augment(text)[0] if text.strip() else text\n",
        "\n",
        "def eda_random_swap(text: str, language: str = 'english'):\n",
        "    return swap_aug.augment(text)[0] if text.strip() else text\n",
        "\n",
        "def apply_eda_augmentation(text: str, language: str = 'english', num_aug: int = 4):\n",
        "    \"\"\"\n",
        "    Generate `num_aug` different augmented versions\n",
        "    \"\"\"\n",
        "    augs = [delete_aug, swap_aug, insert_aug]\n",
        "    if language == 'english':\n",
        "        augs.append(english_syn_aug)\n",
        "    augs.append(contextual_aug)\n",
        "\n",
        "    results = []\n",
        "    seen = {text}\n",
        "\n",
        "    for _ in range(num_aug * 2):  # Oversample a bit to guarantee diversity\n",
        "        aug = random.choice(augs)\n",
        "        try:\n",
        "            new_text = aug.augment(text)\n",
        "            if isinstance(new_text, list):\n",
        "                new_text = new_text[0]\n",
        "            if new_text not in seen and len(new_text.split()) > 5:\n",
        "                seen.add(new_text)\n",
        "                results.append(new_text)\n",
        "                if len(results) >= num_aug:\n",
        "                    break\n",
        "        except:\n",
        "            continue\n",
        "    return results if results else [text]"
      ],
      "metadata": {
        "id": "NjbEd0e3c5hG"
      },
      "id": "NjbEd0e3c5hG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== FULL AUGMENT FUNCTION (multiplier) ======================\n",
        "def augment_with_eda(data: pd.DataFrame, multiplier: int = 4):\n",
        "    \"\"\"\n",
        "    Augment ONLY polarized examples with `multiplier` new samples each.\n",
        "    \"\"\"\n",
        "    polarized_data = data[data['polarization'] == 1].copy()\n",
        "    print(f\"Found {len(polarized_data)} polarized examples → generating {multiplier}x each\")\n",
        "\n",
        "    aug_rows = []\n",
        "    for _, row in polarized_data.iterrows():\n",
        "        lang = 'hausa' if row.get('language', '').startswith('hau') else 'english'\n",
        "        aug_texts = apply_eda_augmentation(row['text'], language=lang, num_aug=multiplier)\n",
        "\n",
        "        for aug_text in aug_texts:\n",
        "            aug_rows.append({\n",
        "                'id': f\"{row['id']}_eda_{random.randint(1000,9999)}\",\n",
        "                'text': aug_text,\n",
        "                'polarization': 1,\n",
        "                'language': row.get('language', lang)\n",
        "            })\n",
        "\n",
        "    aug_df = pd.DataFrame(aug_rows)\n",
        "    print(f\"Generated {len(aug_df)} high-quality EDA samples\")\n",
        "    return aug_df\n",
        "\n",
        "def augment_with_eda_batch(data: pd.DataFrame, multiplier: int = 4):\n",
        "    \"\"\"\n",
        "    Augment polarized examples using Batched EDA.\n",
        "    Separates English and Hausa to apply appropriate augmenters efficiently.\n",
        "    \"\"\"\n",
        "    # 1. Split Data by Language\n",
        "    eng_data = data[(data['polarization'] == 1) & (data['language'] == 'eng')].copy()\n",
        "    hau_data = data[(data['polarization'] == 1) & (data['language'] == 'hau')].copy()\n",
        "\n",
        "    print(f\"Found {len(eng_data)} English and {len(hau_data)} Hausa polarized examples.\")\n",
        "\n",
        "    new_rows = []\n",
        "\n",
        "    # --- Define Available Augmenters ---\n",
        "    # Hausa: XLM-R (Sub/Ins), Random (Swap/Del)\n",
        "    hau_augmenters = [contextual_aug, insert_aug, swap_aug, delete_aug]\n",
        "\n",
        "    # English: XLM-R (Sub/Ins), Random (Swap/Del), WordNet\n",
        "    eng_augmenters = [contextual_aug, insert_aug, swap_aug, delete_aug, english_syn_aug]\n",
        "\n",
        "    # --- Helper to process a batch ---\n",
        "    def process_batch(df, augmenters_list, prefix):\n",
        "        if df.empty: return\n",
        "\n",
        "        # Extract lists once\n",
        "        texts = df['text'].tolist()\n",
        "        ids = df['id'].tolist()\n",
        "        langs = df['language'].tolist()\n",
        "\n",
        "        # Loop for the number of requested augmentations (multiplier)\n",
        "        for i in range(multiplier):\n",
        "            # Pick an augmenter for this round\n",
        "            aug_engine = augmenters_list[i % len(augmenters_list)]\n",
        "\n",
        "            print(f\"  [{prefix}] Round {i+1}/{multiplier}: Applying {type(aug_engine).__name__}...\")\n",
        "\n",
        "            # BATCH INFERENCE\n",
        "            try:\n",
        "                augmented_texts = aug_engine.augment(texts)\n",
        "            except Exception as e:\n",
        "                print(f\"    Error in batch augmentation: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Collect results\n",
        "            # FIX: We now zip 'texts' as well so we have the original text for comparison\n",
        "            for original_id, aug_text, lang, original_txt in zip(ids, augmented_texts, langs, texts):\n",
        "\n",
        "                # Handle case where nlpaug returns a list inside a list\n",
        "                if isinstance(aug_text, list):\n",
        "                    aug_text = aug_text[0]\n",
        "\n",
        "                # Basic validation\n",
        "                # Check if text changed AND is not too short\n",
        "                if aug_text != original_txt and len(aug_text.strip()) > 5:\n",
        "                     new_rows.append({\n",
        "                        'id': f\"{original_id}_eda_{i}_{random.randint(100,999)}\",\n",
        "                        'text': aug_text,\n",
        "                        'polarization': 1,\n",
        "                        'language': lang\n",
        "                    })\n",
        "\n",
        "    # 2. Process Batches\n",
        "    if not eng_data.empty:\n",
        "        process_batch(eng_data, eng_augmenters, \"ENG\")\n",
        "\n",
        "    if not hau_data.empty:\n",
        "        process_batch(hau_data, hau_augmenters, \"HAU\")\n",
        "\n",
        "    result_df = pd.DataFrame(new_rows)\n",
        "    print(f\"Batch EDA complete. Generated {len(result_df)} samples.\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "E4bguNSfc8mb"
      },
      "id": "E4bguNSfc8mb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ls0C6YdrGWxB",
      "metadata": {
        "id": "ls0C6YdrGWxB"
      },
      "source": [
        "### Testing EDA Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== TESTING ON SAMPLE TEXTS ======================\n",
        "sample_texts = [\n",
        "    (\"This is a polarized English statement with multiple words.\", \"english\"),\n",
        "    (\"Ƙungiyar tarayya ce ta zamana kansa a Nigeria.\", \"hausa\"),\n",
        "    (\"We must stand together against injustice and oppression.\", \"english\"),\n",
        "    (\"Ana bukatar gwamnati don taimaka mutane masu dauki abubuwa.\", \"hausa\")\n",
        "]\n",
        "\n",
        "print(\"Testing EDA augmentation functions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, (text, lang) in enumerate(sample_texts):\n",
        "    print(f\"\\nSample {i+1} ({lang}):\\nOriginal:  {text}\\n\")\n",
        "\n",
        "    print(f\"Synonym:   {eda_synonym_replacement(text, lang)}\")\n",
        "    print(f\"Insertion: {eda_random_insertion(text, lang)}\")\n",
        "    print(f\"Deletion:  {eda_random_deletion(text, lang)}\")\n",
        "    print(f\"Swap:      {eda_random_swap(text, lang)}\")\n",
        "\n",
        "    mixed = apply_eda_augmentation(text, lang, num_aug=3)\n",
        "    print(f\"Mixed EDA (3x):\")\n",
        "    for j, t in enumerate(mixed, 1):\n",
        "        print(f\"  {j}. {t}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "7gUFPqmYdA7y"
      },
      "id": "7gUFPqmYdA7y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== APPLY TO YOUR FULL DATA ======================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Applying Batched EDA augmentation...\")\n",
        "\n",
        "# Note: We use fully_augmented_train (Original + BackTrans + Paraphrase)\n",
        "# This creates variations of variations, which is very powerful for small datasets.\n",
        "eda_augmented_data = augment_with_eda_batch(fully_augmented_train, multiplier=4)\n",
        "\n",
        "complete_augmented_train = pd.concat([fully_augmented_train, eda_augmented_data], ignore_index=True)\n",
        "print(f\"Complete augmented training data shape: {complete_augmented_train.shape}\")\n",
        "\n",
        "# Final statistics\n",
        "print(\"\\nComplete Data Statistics:\")\n",
        "print(f\"Total final samples: {len(complete_augmented_train)}\")\n",
        "print(\"\\nFinal Polarization Distribution:\")\n",
        "print(complete_augmented_train['polarization'].value_counts().sort_index())\n",
        "\n",
        "# --- Save the complete augmented dataset ---\n",
        "checkpoint_filename = \"/content/drive/MyDrive/polar/eda_dataset_checkpoint.csv\"\n",
        "complete_augmented_train.to_csv(checkpoint_filename, index=False)\n",
        "eda_augmented_data.to_csv(\"/content/drive/MyDrive/polar/eda_augmented_data.csv\", index=False)\n",
        "print(f\"Data checkpoint saved to: {checkpoint_filename}\")"
      ],
      "metadata": {
        "id": "tHJ_o57bdD-P"
      },
      "id": "tHJ_o57bdD-P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to load the checkpoint back ---\n",
        "\n",
        "try:\n",
        "    complete_augmented_train = pd.read_csv(\"/content/drive/MyDrive/polar/eda_dataset_checkpoint.csv\")\n",
        "    eda_augmented_data = pd.read_csv(\"/content/drive/MyDrive/polar/eda_augmented_data.csv\")\n",
        "    print(f\"\\nSuccessfully loaded data from checkpoint\")\n",
        "    print(f\"Loaded dataset shape: {complete_augmented_train.shape}\")\n",
        "    print(\"Loaded dataset head:\\n\", complete_augmented_train.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: Checkpoint file not found. Please ensure it exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while loading the checkpoint: {e}\")"
      ],
      "metadata": {
        "id": "C0ch72MZlXZL"
      },
      "id": "C0ch72MZlXZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1h--DDLOGYnD",
      "metadata": {
        "id": "1h--DDLOGYnD"
      },
      "source": [
        "## Hausa to English Translation with Silver Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cu0zdTBLHHT5",
      "metadata": {
        "id": "Cu0zdTBLHHT5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "translator_eng = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"facebook/nllb-200-distilled-1.3B\",\n",
        "    src_lang=\"hau_Latn\",\n",
        "    tgt_lang=\"eng_Latn\",\n",
        "    device=device,\n",
        "    dtype=dtype,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "english_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    top_k=None,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    dtype=dtype\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_with_hausa_to_english_silver_labels_batch(\n",
        "    hausa_data: pd.DataFrame,\n",
        "    confidence_threshold: float = 0.85\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Translate Hausa → English → Predict Sentiment → Map to Polarization.\n",
        "    Polarization = Negative + Positive\n",
        "    Neutral = Neutral\n",
        "    \"\"\"\n",
        "    # Create copy and handle missing language column\n",
        "    df = hausa_data.copy()\n",
        "    if 'language' not in df.columns:\n",
        "        df['language'] = 'hau'\n",
        "\n",
        "    hausa_subset = df[df['language'] == 'hau'].copy()\n",
        "\n",
        "    if hausa_subset.empty:\n",
        "        print(\"No Hausa data found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(hausa_subset)} Hausa examples → translating + silver labeling...\")\n",
        "\n",
        "    # Extract texts\n",
        "    hausa_texts = hausa_subset['text'].tolist()\n",
        "\n",
        "    # --- Step 1: Batch Translation ---\n",
        "    print(\"  Batch translating Hausa -> English...\")\n",
        "    try:\n",
        "        # Added truncation=True to fix the length warnings\n",
        "        translated_results = translator_eng(hausa_texts, max_length=512, truncation=True)\n",
        "        translated_texts = [res['translation_text'] for res in translated_results]\n",
        "    except Exception as e:\n",
        "        print(f\"Translation failed: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 2: Batch Classification ---\n",
        "    print(\"  Batch classifying English texts...\")\n",
        "    try:\n",
        "        predictions = english_classifier(\n",
        "            translated_texts,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Classification failed: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- Step 3: Filtering & Logic Mapping ---\n",
        "    silver_rows = []\n",
        "    kept = 0\n",
        "\n",
        "    for orig_row, eng_text, pred in zip(hausa_subset.itertuples(), translated_texts, predictions):\n",
        "        # pred is list of dicts, e.g., [{'label': 'negative', 'score': 0.9}, ...]\n",
        "        # Map labels to scores.\n",
        "        # Note: Model labels are usually 'negative', 'neutral', 'positive'\n",
        "        scores = {p['label']: p['score'] for p in pred}\n",
        "\n",
        "        neg = scores.get('negative', 0.0)\n",
        "        neu = scores.get('neutral', 0.0)\n",
        "        pos = scores.get('positive', 0.0)\n",
        "\n",
        "        # LOGIC: Polarization implies strong opinion (Positive OR Negative)\n",
        "        polarization_score = neg + pos\n",
        "        neutral_score = neu\n",
        "\n",
        "        # Determine label\n",
        "        if polarization_score > neutral_score:\n",
        "            pred_label = 1\n",
        "            confidence = polarization_score\n",
        "        else:\n",
        "            pred_label = 0\n",
        "            confidence = neutral_score\n",
        "\n",
        "        # Filter by confidence\n",
        "        if confidence >= confidence_threshold:\n",
        "            silver_rows.append({\n",
        "                'id': f\"{orig_row.id}_silver_eng\",\n",
        "                'text': eng_text,\n",
        "                'polarization': pred_label,\n",
        "                'language': 'eng',\n",
        "                'source': 'hausa_silver',\n",
        "                'confidence': round(confidence, 4)\n",
        "            })\n",
        "            kept += 1\n",
        "\n",
        "    silver_df = pd.DataFrame(silver_rows)\n",
        "    print(f\"Generated {len(silver_df)} high-confidence silver English examples \"\n",
        "          f\"({kept/len(hausa_subset)*100:.1f}% of Hausa data kept @ {confidence_threshold})\")\n",
        "    return silver_df"
      ],
      "metadata": {
        "id": "CJKEYg4h-Yu-"
      },
      "id": "CJKEYg4h-Yu-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Wy5yA7XAGZSz",
      "metadata": {
        "id": "Wy5yA7XAGZSz"
      },
      "source": [
        "### Testing Hausa to English Translation with Silver Labels\n",
        "\n",
        "Let's test our Hausa to English translation with silver labels function.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing Hausa → English translation + silver polarization labeling:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sample_hausa_texts = [\n",
        "    \"Ƙungiyar tarayya ce ta zamana kansa a Nigeria.\", # Neutral/Factual\n",
        "    \"Ana bukatar gwamnati don taimaka mutane.\",      # Slightly Positive/Demand\n",
        "    \"Mu ba za mu yarda da wannan zalunci ba a ƙasar mu.\", # Strong Negative (Polarized)\n",
        "    \"Ƙasar Nigeria tana da al'umma mai yawa da bambancin addinai.\" # Neutral\n",
        "]\n",
        "\n",
        "# 1. Translate\n",
        "test_trans = translator_eng(sample_hausa_texts, max_length=512, truncation=True)\n",
        "test_eng_texts = [t['translation_text'] for t in test_trans]\n",
        "\n",
        "# 2. Predict\n",
        "test_preds = english_classifier(test_eng_texts)\n",
        "\n",
        "for i, (h_text, e_text, pred) in enumerate(zip(sample_hausa_texts, test_eng_texts, test_preds)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Hausa:       {h_text}\")\n",
        "    print(f\"Translated:  {e_text}\")\n",
        "\n",
        "    # Extract scores based on new model labels\n",
        "    scores = {p['label']: p['score'] for p in pred}\n",
        "    neg = scores.get('negative', 0.0)\n",
        "    neu = scores.get('neutral', 0.0)\n",
        "    pos = scores.get('positive', 0.0)\n",
        "\n",
        "    # Calculate Polarization vs Neutrality\n",
        "    polarization_score = neg + pos\n",
        "    neutral_score = neu\n",
        "\n",
        "    pred_label = 1 if polarization_score > neutral_score else 0\n",
        "    confidence = max(polarization_score, neutral_score)\n",
        "\n",
        "    print(f\"Scores:      Neg={neg:.2f}, Neu={neu:.2f}, Pos={pos:.2f}\")\n",
        "    print(f\"Prediction:  {pred_label} (confidence: {confidence:.3f})\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "qtpALD3IRHhh"
      },
      "id": "qtpALD3IRHhh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY TO FULL HAUSA TRAINING DATA\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Generating high-confidence silver-labeled English data from Hausa train...\")\n",
        "\n",
        "# Use the new BATCH function\n",
        "silver_labeled_data = augment_with_hausa_to_english_silver_labels_batch(\n",
        "    hau_train,                    # original Hausa DataFrame\n",
        "    confidence_threshold=0.95     # Keeps only very clean predictions\n",
        ")\n",
        "\n",
        "print(f\"Silver-labeled English data shape: {silver_labeled_data.shape}\")\n",
        "if len(hau_train) > 0:\n",
        "    print(f\"Kept {len(silver_labeled_data)/len(hau_train)*100:.1f}% of Hausa examples as high-confidence silver data\")\n",
        "\n",
        "# COMBINE WITH CURRENT BEST AUGMENTED DATA\n",
        "final_augmented_train = pd.concat([complete_augmented_train, silver_labeled_data], ignore_index=True)\n",
        "print(f\"\\nFinal augmented training data shape: {final_augmented_train.shape}\")\n",
        "\n",
        "# --- Save the complete augmented dataset as a checkpoint ---\n",
        "checkpoint_filename = \"/content/drive/MyDrive/polar/silver_dataset_checkpoint.csv\"\n",
        "final_augmented_train.to_csv(checkpoint_filename, index=False)\n",
        "silver_labeled_data.to_csv(\"/content/drive/MyDrive/polar/silver_labeled_data.csv\", index=False)\n",
        "print(f\"Data checkpoint saved to: {checkpoint_filename}\")"
      ],
      "metadata": {
        "id": "0MwglqYvRinf"
      },
      "id": "0MwglqYvRinf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL STATISTICS\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL DATASET BREAKDOWN\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_original = len(eng_train) + len(hau_train)\n",
        "print(f\"Original total samples:           {total_original:,}\")\n",
        "print(f\"After all augmentations:          {len(final_augmented_train):,} (+{len(final_augmented_train)/total_original:.2f}x)\")\n",
        "\n",
        "print(\"\\nPolarization Distribution (Final):\")\n",
        "print(final_augmented_train['polarization'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nLanguage Distribution (Final):\")\n",
        "print(final_augmented_train['language'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nPolarization × Language Breakdown:\")\n",
        "print(final_augmented_train.groupby(['language', 'polarization']).size().unstack(fill_value=0))\n",
        "\n",
        "# Augmentation Multiplier on Polarized Class\n",
        "orig_eng_pol = len(eng_train[eng_train['polarization'] == 1])\n",
        "orig_hau_pol = len(hau_train[hau_train['polarization'] == 1])\n",
        "\n",
        "final_eng_pol = len(final_augmented_train[\n",
        "    (final_augmented_train['language'] == 'eng') &\n",
        "    (final_augmented_train['polarization'] == 1)\n",
        "])\n",
        "final_hau_pol = len(final_augmented_train[\n",
        "    (final_augmented_train['language'] == 'hau') &\n",
        "    (final_augmented_train['polarization'] == 1)\n",
        "])\n",
        "\n",
        "print(f\"\\nAugmentation Ratio on Polarized Class:\")\n",
        "print(f\"  English: {final_eng_pol / orig_eng_pol:.2f}× (from {orig_eng_pol} → {final_eng_pol})\")\n",
        "print(f\"  Hausa:   {final_hau_pol / orig_hau_pol:.2f}× (from {orig_hau_pol} → {final_hau_pol})\")"
      ],
      "metadata": {
        "id": "_KXOdQ2mpr1D"
      },
      "id": "_KXOdQ2mpr1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to load the checkpoint back ---\n",
        "\n",
        "try:\n",
        "    final_augmented_train = pd.read_csv(\"/content/drive/MyDrive/polar/silver_dataset_checkpoint.csv\")\n",
        "    silver_labeled_data = pd.read_csv(\"/content/drive/MyDrive/polar/silver_labeled_data.csv\")\n",
        "    print(f\"\\nSuccessfully loaded data from checkpoint\")\n",
        "    print(f\"Loaded dataset shape: {final_augmented_train.shape}\")\n",
        "    print(\"Loaded dataset head:\\n\", final_augmented_train.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: Checkpoint file not found. Please ensure it exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while loading the checkpoint: {e}\")"
      ],
      "metadata": {
        "id": "eofLarSGnqWq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eofLarSGnqWq"
    },
    {
      "cell_type": "markdown",
      "id": "UXicNDRxGcSc",
      "metadata": {
        "id": "UXicNDRxGcSc"
      },
      "source": [
        "## Contextual Word Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p5LnscfxHFbd",
      "metadata": {
        "id": "p5LnscfxHFbd"
      },
      "outputs": [],
      "source": [
        "if 'llm_paraphrase_pipeline' in locals() and llm_paraphrase_pipeline is not None:\n",
        "    print(\"Using existing pipeline model...\")\n",
        "    infiller_model = llm_paraphrase_pipeline.model\n",
        "    infiller_tokenizer = llm_paraphrase_pipeline.tokenizer\n",
        "else:\n",
        "    print(\"Loading mT5 manually...\")\n",
        "    infiller_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-large\")\n",
        "    infiller_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        \"google/mt5-large\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        infiller_model = infiller_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infill_masked_text(masked_texts: List[str]) -> List[str]:\n",
        "    # Filter out empty or unmasked texts to avoid errors and simplify processing\n",
        "    valid_indices = [i for i, text in enumerate(masked_texts) if \"<extra_id_\" in text and text.strip()]\n",
        "    if not valid_indices:\n",
        "        return list(masked_texts) # Return originals if no valid masks\n",
        "\n",
        "    texts_to_process = [masked_texts[i] for i in valid_indices]\n",
        "\n",
        "    inputs = infiller_tokenizer(\n",
        "        texts_to_process,  # Pass a list for batch tokenization\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,      # Important for batching\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(infiller_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = infiller_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    generated_spans_batch = infiller_tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
        "\n",
        "    final_texts = list(masked_texts) # Start with copies of original masked_texts\n",
        "\n",
        "    for i, original_masked_text_in_batch in enumerate(texts_to_process):\n",
        "        # The index in the original batch `masked_texts`\n",
        "        original_idx = valid_indices[i]\n",
        "        generated_span = generated_spans_batch[i]\n",
        "\n",
        "        current_final_text = original_masked_text_in_batch\n",
        "        matches = re.findall(r\"<extra_id_(\\d+)>(.*?)(?=<extra_id_|</s>|$)\", generated_span)\n",
        "\n",
        "        for idx_str, content in matches:\n",
        "            sentinel = f\"<extra_id_{idx_str}>\"\n",
        "            content = content.strip()\n",
        "            if sentinel in current_final_text:\n",
        "                current_final_text = current_final_text.replace(sentinel, content, 1)\n",
        "\n",
        "        current_final_text = re.sub(r'\\s+', ' ', current_final_text).strip()\n",
        "        final_texts[original_idx] = current_final_text # Update the text at its original position\n",
        "\n",
        "    return final_texts"
      ],
      "metadata": {
        "id": "eqWMiA9DRufV"
      },
      "id": "eqWMiA9DRufV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper: Masking (CPU) ---\n",
        "def mask_words_contextual(text: str, mask_ratio: float = 0.15) -> str:\n",
        "    words = text.split()\n",
        "    if len(words) < 5:\n",
        "        return text\n",
        "\n",
        "    n_words = len(words)\n",
        "    n_to_mask = max(1, int(n_words * mask_ratio))\n",
        "    indices = sorted(random.sample(range(n_words), n_to_mask))\n",
        "\n",
        "    spans = []\n",
        "    i = 0\n",
        "    while i < len(indices):\n",
        "        start = indices[i]\n",
        "        end = start\n",
        "        while (i + 1 < len(indices) and indices[i + 1] == indices[i] + 1 and end - start < 10):\n",
        "            i += 1\n",
        "            end = indices[i]\n",
        "        spans.append((start, end + 1))\n",
        "        i += 1\n",
        "\n",
        "    result = []\n",
        "    prev_end = 0\n",
        "    for idx, (start, end) in enumerate(spans):\n",
        "        result.extend(words[prev_end:start])\n",
        "        result.append(f\"<extra_id_{idx}>\")\n",
        "        prev_end = end\n",
        "    result.extend(words[prev_end:])\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# --- Helper: Batch Stitching (CPU) ---\n",
        "def stitch_batch(original_masked_texts, generated_spans):\n",
        "    final_texts = []\n",
        "    for masked_text, gen_span in zip(original_masked_texts, generated_spans):\n",
        "        current_text = masked_text\n",
        "        # Extract <extra_id_X> content\n",
        "        matches = re.findall(r\"<extra_id_(\\d+)>(.*?)(?=<extra_id_|</s>|$)\", gen_span)\n",
        "\n",
        "        for idx_str, content in matches:\n",
        "            sentinel = f\"<extra_id_{idx_str}>\"\n",
        "            if sentinel in current_text:\n",
        "                current_text = current_text.replace(sentinel, content.strip(), 1)\n",
        "\n",
        "        final_texts.append(re.sub(r'\\s+', ' ', current_text).strip())\n",
        "    return final_texts"
      ],
      "metadata": {
        "id": "od0_ZqzwfxiI"
      },
      "id": "od0_ZqzwfxiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- 2. Multilingual Similarity Checker -------------------\n",
        "similarity_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "def compute_similarity(text1: str, text2: str, threshold: float = 0.8) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity using multilingual MPNet (filters bad infills).\n",
        "    Returns score; use > threshold to accept.\n",
        "    \"\"\"\n",
        "    if not text1.strip() or not text2.strip():\n",
        "        return 0.0\n",
        "\n",
        "    emb1 = similarity_model.encode(text1)\n",
        "    emb2 = similarity_model.encode(text2)\n",
        "    sim = util.cos_sim(emb1, emb2).item()\n",
        "    return sim"
      ],
      "metadata": {
        "id": "6fqaUAR5dcpv"
      },
      "id": "6fqaUAR5dcpv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_with_contextual_masking(\n",
        "    data: pd.DataFrame,\n",
        "    multiplier: int = 3,\n",
        "    sim_threshold: float = 0.70\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    # Filter for polarized data\n",
        "    polarized = data[data['polarization'] == 1].copy()\n",
        "    print(f\"Applying mT5 contextual masking to {len(polarized)} polarized examples (Target: {multiplier} augs per row)\")\n",
        "\n",
        "    new_rows = []\n",
        "\n",
        "    for _, row in tqdm(polarized.iterrows(), total=len(polarized)):\n",
        "        text = row['text'].strip()\n",
        "        lang = row.get('language', 'eng')\n",
        "\n",
        "        # Counter for successful augmentations for THIS row\n",
        "        success_count = 0\n",
        "\n",
        "        # Safety limit: try at most (multiplier * 3) times to find good ones\n",
        "        # to prevent infinite loops on difficult sentences\n",
        "        attempts = 0\n",
        "        max_attempts = multiplier * 3\n",
        "\n",
        "        while success_count < multiplier and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "\n",
        "            try:\n",
        "                # 1. Mask\n",
        "                masked = mask_words_contextual(text)\n",
        "                if \"<extra_id_\" not in masked:\n",
        "                    continue\n",
        "\n",
        "                # 2. Infill (using the corrected function from previous step)\n",
        "                aug_text = infill_masked_text(masked)\n",
        "\n",
        "                # 3. Validate\n",
        "                # Basic length check to save time on similarity calc\n",
        "                if len(aug_text.split()) < 5:\n",
        "                    continue\n",
        "\n",
        "                # Similarity check\n",
        "                sim = compute_similarity(text, aug_text)\n",
        "\n",
        "                # 4. Filter Logic\n",
        "                if (sim >= sim_threshold and\n",
        "                    sim != 1.0 and  # Add this condition to reject identical augmentations\n",
        "                    aug_text != text and\n",
        "                    # Ensure we didn't just delete words (augmentation shouldn't be a substring of original)\n",
        "                    aug_text.lower() not in text.lower()):\n",
        "\n",
        "                    new_rows.append({\n",
        "                        'id': f\"{row['id']}_mt5_{success_count}_{random.randint(100,999)}\",\n",
        "                        'text': aug_text,\n",
        "                        'polarization': 1,\n",
        "                        'language': lang\n",
        "                    })\n",
        "\n",
        "                    success_count += 1\n",
        "            except Exception as e:\n",
        "                # print(f\"Error: {e}\") # Uncomment for debugging\n",
        "                continue\n",
        "\n",
        "    result_df = pd.DataFrame(new_rows)\n",
        "    print(f\"Done! Generated {len(result_df)} high-quality mT5 augmentations.\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "I6H3K8hfdkaS"
      },
      "id": "I6H3K8hfdkaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_with_contextual_masking_batch(\n",
        "    data: pd.DataFrame,\n",
        "    multiplier: int = 2,\n",
        "    sim_threshold: float = 0.80,\n",
        "    mask_ratio: float = 0.15,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_path: str = None,\n",
        "    save_every_batches: int = 50\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    # 1. Filter Data\n",
        "    polarized = data[data['polarization'] == 1].copy()\n",
        "    print(f\"Applying Batch mT5 masking to {len(polarized)} examples (Target: {multiplier}x each)\")\n",
        "\n",
        "    # 2. Pre-generate Masked Inputs\n",
        "    tasks = []\n",
        "    for _, row in polarized.iterrows():\n",
        "        text = row['text'].strip()\n",
        "        if len(text.split()) < 4: continue\n",
        "\n",
        "        for i in range(multiplier):\n",
        "            masked = mask_words_contextual(text, mask_ratio=mask_ratio)\n",
        "            if \"<extra_id_\" in masked:\n",
        "                tasks.append({\n",
        "                    'orig_id': row['id'],\n",
        "                    'orig_text': text,\n",
        "                    'masked_text': masked,\n",
        "                    'language': row.get('language', 'eng'),\n",
        "                    'iter': i\n",
        "                })\n",
        "\n",
        "    print(f\"Generated {len(tasks)} masked candidates. Processing in batches of {batch_size}...\")\n",
        "\n",
        "    valid_rows = []\n",
        "    batch_count = 0\n",
        "\n",
        "    # 3. Process in Batches\n",
        "    for i in range(0, len(tasks), batch_size):\n",
        "        batch = tasks[i : i + batch_size]\n",
        "        batch_count += 1\n",
        "\n",
        "        # Extract lists\n",
        "        batch_masked = [b['masked_text'] for b in batch]\n",
        "        batch_orig = [b['orig_text'] for b in batch]\n",
        "\n",
        "        # --- A. Batch Infilling (mT5) ---\n",
        "        try:\n",
        "            inputs = infiller_tokenizer(\n",
        "                batch_masked,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(infiller_model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output_ids = infiller_model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=128,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.9,\n",
        "                    top_p=0.95\n",
        "                )\n",
        "\n",
        "            generated_spans = infiller_tokenizer.batch_decode(output_ids, skip_special_tokens=False)\n",
        "            batch_filled = stitch_batch(batch_masked, generated_spans)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Batch generation failed: {e}\")\n",
        "            continue\n",
        "\n",
        "        # --- B. Batch Similarity ---\n",
        "        try:\n",
        "            orig_embs = similarity_model.encode(batch_orig, convert_to_tensor=True, show_progress_bar=False)\n",
        "            aug_embs = similarity_model.encode(batch_filled, convert_to_tensor=True, show_progress_bar=False)\n",
        "            sim_scores = util.pairwise_cos_sim(orig_embs, aug_embs)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Batch similarity failed: {e}\")\n",
        "            continue\n",
        "\n",
        "        # --- C. Filter and Collect ---\n",
        "        for j, task in enumerate(batch):\n",
        "            aug_text = batch_filled[j]\n",
        "            score = sim_scores[j].item()\n",
        "            orig_text = task['orig_text']\n",
        "\n",
        "            if (score >= sim_threshold and\n",
        "                score < 0.99 and\n",
        "                len(aug_text.split()) >= 4 and\n",
        "                aug_text.lower() != orig_text.lower() and\n",
        "                aug_text.lower() not in orig_text.lower()):\n",
        "\n",
        "                valid_rows.append({\n",
        "                    'id': f\"{task['orig_id']}_mt5_{task['iter']}_{random.randint(100,999)}\",\n",
        "                    'text': aug_text,\n",
        "                    'polarization': 1,\n",
        "                    'language': task['language']\n",
        "                })\n",
        "\n",
        "        # --- D. CHECKPOINT SAVING ---\n",
        "        if checkpoint_path and (batch_count % save_every_batches == 0):\n",
        "            if len(valid_rows) > 0:\n",
        "                temp_df = pd.DataFrame(valid_rows)\n",
        "                temp_df.to_csv(checkpoint_path, index=False)\n",
        "                print(f\"  [Checkpoint] Saved {len(temp_df)} generated samples to {checkpoint_path}\")\n",
        "\n",
        "        # Print progress\n",
        "        if (i // batch_size) % 20 == 0:\n",
        "            print(f\"Processed {i}/{len(tasks)} candidates... Found {len(valid_rows)} valid so far.\")\n",
        "\n",
        "    result_df = pd.DataFrame(valid_rows)\n",
        "\n",
        "    # Final save\n",
        "    if checkpoint_path and not result_df.empty:\n",
        "        result_df.to_csv(checkpoint_path, index=False)\n",
        "        print(f\"Final checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    print(f\"Done! Generated {len(result_df)} high-quality mT5 augmentations.\")\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "zK1gBrmTAn7M"
      },
      "id": "zK1gBrmTAn7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "OsFrPlQ2Gebf",
      "metadata": {
        "id": "OsFrPlQ2Gebf"
      },
      "source": [
        "### Testing Contextual Word Masking\n",
        "\n",
        "Let's test our contextual word masking augmentation functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vxnYPOLQH-g6",
      "metadata": {
        "id": "vxnYPOLQH-g6"
      },
      "outputs": [],
      "source": [
        "print(\"Testing contextual word masking + umT5 infilling on sample texts:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sample_texts = [\n",
        "    \"This is a polarized English statement with multiple words for testing.\",\n",
        "    \"Ƙungiyar tarayya ce ta zamana kansa a Nigeria da alƙawarin da ke saman sa.\",\n",
        "    \"We must stand together against injustice and oppression in our society.\",\n",
        "    \"Ana bukatar gwamnati don taimaka mutane masu dauki abubuwa da basira.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Original:   {text}\")\n",
        "\n",
        "    masked = mask_words_contextual(text, mask_ratio=0.15)\n",
        "    print(f\"Masked:     {masked}\")\n",
        "\n",
        "    infilled = infill_masked_text(masked)\n",
        "    print(f\"Infilled:   {infilled}\")\n",
        "\n",
        "    sim = compute_similarity(text, infilled)\n",
        "    status = \"ACCEPTED\" if sim >= 0.8 else \"REJECTED\"\n",
        "    print(f\"Similarity: {sim:.3f} → {status}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY TO FULL FINAL AUGMENTED TRAINING DATA\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Applying high-quality BATCHED contextual masking augmentation...\")\n",
        "\n",
        "# Define a path for the intermediate/partial results\n",
        "partial_save_path = \"/content/drive/MyDrive/polar/partial_contextual_aug.csv\"\n",
        "\n",
        "contextual_augmented_data = augment_with_contextual_masking_batch(\n",
        "    final_augmented_train,\n",
        "    mask_ratio=0.15,\n",
        "    sim_threshold=0.80,\n",
        "    multiplier=2,\n",
        "    batch_size=32,\n",
        "    checkpoint_path=partial_save_path,\n",
        "    save_every_batches=50\n",
        ")\n",
        "\n",
        "print(f\"Contextual masking augmented data shape: {contextual_augmented_data.shape}\")\n",
        "\n",
        "# FINAL COMBINATION\n",
        "complete_augmentation_dataset = pd.concat(\n",
        "    [final_augmented_train, contextual_augmented_data],\n",
        "    ignore_index=True\n",
        ")\n",
        "print(f\"\\nCOMPLETE FINAL DATASET shape: {complete_augmentation_dataset.shape}\")\n",
        "print(\"\\nFinal Polarization Distribution:\")\n",
        "print(complete_augmentation_dataset['polarization'].value_counts().sort_index())\n",
        "\n",
        "# --- Save the complete augmented dataset as a final checkpoint ---\n",
        "checkpoint_filename = \"/content/drive/MyDrive/polar/mask_dataset_checkpoint.csv\"\n",
        "complete_augmentation_dataset.to_csv(checkpoint_filename, index=False)\n",
        "contextual_augmented_data.to_csv(\"/content/drive/MyDrive/polar/contextual_augmented_data.csv\", index=False)\n",
        "print(f\"Final Data checkpoint saved to: {checkpoint_filename}\")"
      ],
      "metadata": {
        "id": "0oU_DqakaK4Y"
      },
      "id": "0oU_DqakaK4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Code to load the checkpoint back (for later use or if notebook restarts) ---\n",
        "\n",
        "try:\n",
        "    complete_augmentation_dataset = pd.read_csv(\"/content/drive/MyDrive/polar/mask_dataset_checkpoint.csv\")\n",
        "    contextual_augmented_data = pd.read_csv(\"/content/drive/MyDrive/polar/contextual_augmented_data.csv\")\n",
        "    print(f\"\\nSuccessfully loaded data from checkpoint\")\n",
        "    print(f\"Loaded dataset shape: {complete_augmentation_dataset.shape}\")\n",
        "    print(\"Loaded dataset head:\\n\", complete_augmentation_dataset.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: Checkpoint file not found. Please ensure it exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while loading the checkpoint: {e}\")"
      ],
      "metadata": {
        "id": "udAtxNOUn4o0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "udAtxNOUn4o0"
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL STATISTICS & BREAKDOWN\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPLETE AUGMENTATION PIPELINE STATISTICS (Dec 2025 SOTA)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "total_original = len(eng_train) + len(hau_train)\n",
        "print(f\"Original total samples:                     {total_original:,}\")\n",
        "print(f\"After ALL augmentations:                    {len(complete_augmentation_dataset):,} \"\n",
        "      f\"(+{len(complete_augmentation_dataset)/total_original:.2f}x)\")\n",
        "\n",
        "print(\"\\nBreakdown of Augmentation Techniques:\")\n",
        "print(f\"  • Back-translation                        → {len(augmented_data):,}\")\n",
        "print(f\"  • LLM paraphrasing                        → {len(llm_augmented_data):,}\")\n",
        "print(f\"  • EDA (XLM-R contextual)                  → {len(eda_augmented_data):,}\")\n",
        "print(f\"  • Hausa→Eng silver labels (NLLB + RoBERTa)→ {len(silver_labeled_data):,}\")\n",
        "print(f\"  • Contextual masking (mT5 + MPNet filter) → {len(contextual_augmented_data):,}\")\n",
        "print(f\"  → TOTAL FINAL SAMPLES:                    {len(complete_augmentation_dataset):,}\")\n",
        "\n",
        "print(\"\\nFinal Language Distribution:\")\n",
        "print(complete_augmentation_dataset['language'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nFinal Polarization Distribution:\")\n",
        "print(complete_augmentation_dataset['polarization'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nPolarization × Language Breakdown:\")\n",
        "print(complete_augmentation_dataset.groupby(['language', 'polarization']).size().unstack(fill_value=0))\n",
        "\n",
        "# AUGMENTATION RATIO ON POLARIZED CLASS (THE MOST IMPORTANT METRIC)\n",
        "orig_eng_pol = len(eng_train[eng_train['polarization'] == 1])\n",
        "orig_hau_pol = len(hau_train[hau_train['polarization'] == 1])\n",
        "\n",
        "final_eng_pol = len(complete_augmentation_dataset[\n",
        "    (complete_augmentation_dataset['language'] == 'eng') &\n",
        "    (complete_augmentation_dataset['polarization'] == 1)\n",
        "])\n",
        "final_hau_pol = len(complete_augmentation_dataset[\n",
        "    (complete_augmentation_dataset['language'] == 'hau') &\n",
        "    (complete_augmentation_dataset['polarization'] == 1)\n",
        "])\n",
        "\n",
        "print(f\"\\nAUGMENTATION MULTIPLIER ON POLARIZED CLASS (Target: 10–15x):\")\n",
        "print(f\"  English polarized: {orig_eng_pol:,} → {final_eng_pol:,} \"\n",
        "      f\"({final_eng_pol/orig_eng_pol:.2f}×)\")\n",
        "print(f\"  Hausa polarized:   {orig_hau_pol:,} → {final_hau_pol:,} \"\n",
        "      f\"({final_hau_pol/orig_hau_pol:.2f}×)\")"
      ],
      "metadata": {
        "id": "xk_TzDDBaRhX"
      },
      "id": "xk_TzDDBaRhX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "TC4rtWdeGfy1",
      "metadata": {
        "id": "TC4rtWdeGfy1"
      },
      "source": [
        "## Saving Augmented Dataset\n",
        "\n",
        "Let's save our complete augmented dataset for use in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "McqMiGlKHJQv",
      "metadata": {
        "id": "McqMiGlKHJQv"
      },
      "outputs": [],
      "source": [
        "# Save the complete augmented dataset\n",
        "output_dir = \"/content/drive/MyDrive/polar/augmented_data_v1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save the complete augmented dataset\n",
        "augmented_file_path = os.path.join(output_dir, \"complete_augmented_train.csv\")\n",
        "complete_augmentation_dataset.to_csv(augmented_file_path, index=False)\n",
        "print(f\"Complete augmented dataset saved to: {augmented_file_path}\")\n",
        "\n",
        "combined_train = pd.concat([eng_train, hau_train], ignore_index=True)\n",
        "\n",
        "# Also save separate files for each augmentation technique for analysis\n",
        "augmentation_summary = {\n",
        "    'original': len(combined_train),\n",
        "    'back_translation': len(augmented_data),\n",
        "    'llm_paraphrasing': len(llm_augmented_data),\n",
        "    'eda': len(eda_augmented_data),\n",
        "    'silver_labeled_english': len(silver_labeled_data),\n",
        "    'contextual_masking': len(contextual_augmented_data),\n",
        "    'total': len(complete_augmentation_dataset)\n",
        "}\n",
        "\n",
        "# Save augmentation summary\n",
        "summary_file_path = os.path.join(output_dir, \"augmentation_summary.csv\")\n",
        "pd.DataFrame([augmentation_summary]).to_csv(summary_file_path, index=False)\n",
        "print(f\"Augmentation summary saved to: {summary_file_path}\")\n",
        "\n",
        "print(\"\\nAugmentation Summary:\")\n",
        "for technique, count in augmentation_summary.items():\n",
        "    print(f\"{technique.replace('_', ' ').title()}: {count}\")\n",
        "\n",
        "# Save language-specific datasets\n",
        "hausa_augmented = complete_augmentation_dataset[complete_augmentation_dataset['language'] == 'hau']\n",
        "english_augmented = complete_augmentation_dataset[complete_augmentation_dataset['language'] == 'eng']\n",
        "\n",
        "hausa_file_path = os.path.join(output_dir, \"hausa_augmented_train.csv\")\n",
        "english_file_path = os.path.join(output_dir, \"english_augmented_train.csv\")\n",
        "\n",
        "hausa_augmented.to_csv(hausa_file_path, index=False)\n",
        "english_augmented.to_csv(english_file_path, index=False)\n",
        "\n",
        "print(f\"\\nLanguage-specific datasets saved:\")\n",
        "print(f\"Hausa: {hausa_file_path}\")\n",
        "print(f\"English: {english_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKmgxiOE8pEr"
      },
      "id": "ZKmgxiOE8pEr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s8ENb3vIJACH"
      },
      "id": "s8ENb3vIJACH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subtask 2: Polarization Type Classification\n",
        "Multi-label classification to identify the target of polarization as one of the following categories: Gender/Sexual, Political, Religious, Racial/Ethnic, or Other.\n",
        "For this task we will load the data for subtask 2."
      ],
      "metadata": {
        "id": "5Qi53eheGIbu"
      },
      "id": "5Qi53eheGIbu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "nOuDpDRjKeiq"
      },
      "id": "nOuDpDRjKeiq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaWIvnYv0rV2"
      },
      "outputs": [],
      "source": [
        "eng_train_2 = pd.read_csv('subtask2/train/eng.csv')\n",
        "hau_train_2 = pd.read_csv('subtask2/train/hau.csv')\n",
        "\n",
        "eng_train_2['language'] = 'eng'\n",
        "hau_train_2['language'] = 'hau'\n",
        "\n",
        "full_train_subtask2 = pd.concat([eng_train_2, hau_train_2], ignore_index=True)"
      ],
      "id": "YaWIvnYv0rV2"
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dev_2 = pd.read_csv('subtask2/dev/eng.csv')\n",
        "hau_dev_2 = pd.read_csv('subtask2/dev/hau.csv')\n",
        "\n",
        "eng_dev_2['language'] = 'eng'\n",
        "hau_dev_2['language'] = 'hau'\n",
        "\n",
        "dev = pd.concat([eng_dev_2, hau_dev_2], ignore_index=True)"
      ],
      "metadata": {
        "id": "8-yJqMQJKzM1"
      },
      "id": "8-yJqMQJKzM1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "4JbPpW16NKFl"
      },
      "id": "4JbPpW16NKFl"
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_subtask2['text'] = full_train_subtask2['text'].apply(preprocess)\n",
        "dev['text'] = dev['text'].apply(preprocess)"
      ],
      "metadata": {
        "id": "gnvCwwwdNG2y"
      },
      "id": "gnvCwwwdNG2y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_columns_subtask2 = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
        "full_train_subtask2['label_sum'] = full_train_subtask2[label_columns_subtask2].sum(axis=1)\n",
        "\n",
        "stratify_col_name = 'label_sum'\n",
        "\n",
        "# Initialize stratify_group with label_sum\n",
        "full_train_subtask2['stratify_group'] = full_train_subtask2[stratify_col_name].copy()\n",
        "\n",
        "# Identify classes with only one instance\n",
        "value_counts = full_train_subtask2['stratify_group'].value_counts()\n",
        "single_instance_labels = value_counts[value_counts == 1].index\n",
        "\n",
        "# If there are single instance labels, merge them into the most frequent category\n",
        "if not single_instance_labels.empty:\n",
        "    most_frequent_label = value_counts.idxmax()\n",
        "    full_train_subtask2.loc[full_train_subtask2['stratify_group'].isin(single_instance_labels), 'stratify_group'] = most_frequent_label\n",
        "\n",
        "train, val = train_test_split(full_train_subtask2, test_size=0.2,\n",
        "                              stratify=full_train_subtask2['stratify_group'],\n",
        "                              random_state=42)\n",
        "\n",
        "train = train.drop(columns=['label_sum', 'stratify_group'])\n",
        "val = val.drop(columns=['label_sum', 'stratify_group'])\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "xCXDlMEYKtM6"
      },
      "id": "xCXDlMEYKtM6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, labels=None, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False,\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_tensors='pt'\n",
        "                                  )\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # Only add labels if they are provided\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item"
      ],
      "metadata": {
        "id": "I6S2S6QBDKzw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "I6S2S6QBDKzw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup"
      ],
      "metadata": {
        "id": "KmgIhl4CLmX5"
      },
      "id": "KmgIhl4CLmX5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(),\n",
        "                                    tokenizer,\n",
        "                                    train[['gender/sexual',\n",
        "                                           'political',\n",
        "                                           'religious',\n",
        "                                           'racial/ethnic',\n",
        "                                           'other'\n",
        "                                           ]].values.tolist()\n",
        "                                    )\n",
        "\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(),\n",
        "                                  tokenizer,\n",
        "                                  val[['gender/sexual',\n",
        "                                       'political',\n",
        "                                       'religious',\n",
        "                                       'racial/ethnic',\n",
        "                                       'other'\n",
        "                                       ]].values.tolist()\n",
        "                                  )\n",
        "\n",
        "dev_dataset = PolarizationDataset(dev['text'].tolist(),\n",
        "                                  tokenizer=tokenizer\n",
        "                                  )"
      ],
      "metadata": {
        "id": "u1_KYsG68nxI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "u1_KYsG68nxI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           num_labels=5,\n",
        "                                                           problem_type=\"multi_label_classification\"\n",
        "                                                           ) # 5 labels"
      ],
      "metadata": {
        "id": "cdiYJyr08bw2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cdiYJyr08bw2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Ensure p.predictions is the actual logits tensor/array\n",
        "    if isinstance(p.predictions, tuple):\n",
        "        logits = p.predictions[0]\n",
        "    else:\n",
        "        logits = p.predictions\n",
        "\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(logits))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ArVWKwze2mtS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ArVWKwze2mtS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "8LWUIMqxLsaE"
      },
      "id": "8LWUIMqxLsaE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "Qd3QPyfc2RKE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Qd3QPyfc2RKE"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "GhbJPYzRwIOh"
      },
      "id": "GhbJPYzRwIOh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 2: {eval_results['eval_f1_macro']}\")"
      ],
      "metadata": {
        "id": "rOT0gjBNQX_v"
      },
      "id": "rOT0gjBNQX_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "P7s9cIXHLx20"
      },
      "id": "P7s9cIXHLx20"
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dev_2_dataset = PolarizationDataset(dev[dev['language'] == 'eng']['text'].tolist(), tokenizer)\n",
        "hau_dev_2_dataset = PolarizationDataset(dev[dev['language'] == 'hau']['text'].tolist(), tokenizer)\n",
        "\n",
        "ids_eng_2 = dev[dev['language'] == 'eng']['id'].tolist()\n",
        "ids_hau_2 = dev[dev['language'] == 'hau']['id'].tolist()\n",
        "\n",
        "original_label_order_in_preds = ['gender/sexual',\n",
        "                                 'political',\n",
        "                                 'religious',\n",
        "                                 'racial/ethnic',\n",
        "                                 'other'\n",
        "                                 ]\n",
        "\n",
        "desired_output_label_order = ['political',\n",
        "                              'racial/ethnic',\n",
        "                              'religious',\n",
        "                              'gender/sexual',\n",
        "                              'other'\n",
        "                              ]\n",
        "\n",
        "label_to_pred_index = {name: original_label_order_in_preds.index(name) for name in original_label_order_in_preds}\n",
        "\n",
        "# Define the output directory and file name for Subtask 2\n",
        "output_dir_subtask2 = 'subtask_2'\n",
        "os.makedirs(output_dir_subtask2, exist_ok=True)\n",
        "\n",
        "# --- Generate predictions for English ---\n",
        "print(\"Generating predictions for English (Subtask 2)...\")\n",
        "predictions_eng_2 = trainer.predict(eng_dev_2_dataset)\n",
        "probs_eng_2 = torch.sigmoid(torch.from_numpy(predictions_eng_2.predictions[0]))\n",
        "predicted_labels_eng_2 = (probs_eng_2 > 0.5).int().numpy()\n",
        "\n",
        "submission_df_eng_2 = pd.DataFrame({\n",
        "    'id': ids_eng_2\n",
        "})\n",
        "\n",
        "for col_name in desired_output_label_order:\n",
        "    pred_idx = label_to_pred_index[col_name]\n",
        "    submission_df_eng_2[col_name] = predicted_labels_eng_2[:, pred_idx]\n",
        "\n",
        "output_filename_eng_2 = os.path.join(output_dir_subtask2, 'pred_eng.csv')\n",
        "submission_df_eng_2.to_csv(output_filename_eng_2, index=False)\n",
        "\n",
        "print(f\"English (Subtask 2) predictions saved to {output_filename_eng_2}\")\n",
        "print(submission_df_eng_2.head())\n",
        "\n",
        "# --- Generate predictions for Hausa ---\n",
        "print(\"Generating predictions for Hausa (Subtask 2)...\")\n",
        "predictions_hau_2 = trainer.predict(hau_dev_2_dataset)\n",
        "probs_hau_2 = torch.sigmoid(torch.from_numpy(predictions_hau_2.predictions[0]))\n",
        "predicted_labels_hau_2 = (probs_hau_2 > 0.5).int().numpy()\n",
        "\n",
        "submission_df_hau_2 = pd.DataFrame({\n",
        "    'id': ids_hau_2\n",
        "})\n",
        "\n",
        "for col_name in desired_output_label_order:\n",
        "    pred_idx = label_to_pred_index[col_name]\n",
        "    submission_df_hau_2[col_name] = predicted_labels_hau_2[:, pred_idx]\n",
        "\n",
        "output_filename_hau_2 = os.path.join(output_dir_subtask2, 'pred_hau.csv')\n",
        "submission_df_hau_2.to_csv(output_filename_hau_2, index=False)\n",
        "\n",
        "print(f\"Hausa (Subtask 2) predictions saved to {output_filename_hau_2}\")\n",
        "print(submission_df_hau_2.head())"
      ],
      "metadata": {
        "id": "DlCAzGILKx23"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DlCAzGILKx23"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content && zip -r /content/subtask_2.zip subtask_2\n",
        "files.download(\"/content/subtask_2.zip\")"
      ],
      "metadata": {
        "id": "65WaacYTNAi0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "65WaacYTNAi0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subtask 3: Manifestation Identification\n",
        "Multi-label classification to classify how polarization is expressed, with multiple possible labels including Vilification, Extreme Language, Stereotype, Invalidation, Lack of Empathy, and Dehumanization.\n",
        "\n"
      ],
      "metadata": {
        "id": "UL1uE8llIgTQ"
      },
      "id": "UL1uE8llIgTQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "a1pLF773Ta0f"
      },
      "id": "a1pLF773Ta0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCz20cgl-K3t"
      },
      "outputs": [],
      "source": [
        "eng_train_3 = pd.read_csv('subtask3/train/eng.csv')\n",
        "hau_train_3 = pd.read_csv('subtask3/train/hau.csv')\n",
        "\n",
        "eng_train_3['language'] = 'eng'\n",
        "hau_train_3['language'] = 'hau'\n",
        "\n",
        "full_train_subtask3 = pd.concat([eng_train_3, hau_train_3], ignore_index=True)"
      ],
      "id": "nCz20cgl-K3t"
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dev_3 = pd.read_csv('subtask3/dev/eng.csv')\n",
        "hau_dev_3 = pd.read_csv('subtask3/dev/hau.csv')\n",
        "\n",
        "eng_dev_3['language'] = 'eng'\n",
        "hau_dev_3['language'] = 'hau'\n",
        "\n",
        "dev = pd.concat([eng_dev_3, hau_dev_3], ignore_index=True)"
      ],
      "metadata": {
        "id": "zDWo9flVL5hA"
      },
      "id": "zDWo9flVL5hA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "y5eoy_biNQJx"
      },
      "id": "y5eoy_biNQJx"
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_subtask3['text'] = full_train_subtask3['text'].apply(preprocess)\n",
        "dev['text'] = dev['text'].apply(preprocess)"
      ],
      "metadata": {
        "id": "vwgh3o_WM8m1"
      },
      "id": "vwgh3o_WM8m1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_columns_subtask3 = ['stereotype',\n",
        "                          'vilification',\n",
        "                          'dehumanization',\n",
        "                          'extreme_language',\n",
        "                          'lack_of_empathy',\n",
        "                          'invalidation'\n",
        "                          ]\n",
        "\n",
        "full_train_subtask3['label_sum'] = full_train_subtask3[label_columns_subtask3].sum(axis=1)\n",
        "\n",
        "stratify_col_name = 'label_sum'\n",
        "\n",
        "# Initialize stratify_group with label_sum\n",
        "full_train_subtask3['stratify_group'] = full_train_subtask3[stratify_col_name].copy()\n",
        "\n",
        "# Identify classes with only one instance\n",
        "value_counts = full_train_subtask3['stratify_group'].value_counts()\n",
        "single_instance_labels = value_counts[value_counts == 1].index\n",
        "\n",
        "# If there are single instance labels, merge them into the most frequent category\n",
        "if not single_instance_labels.empty:\n",
        "    most_frequent_label = value_counts.idxmax()\n",
        "    full_train_subtask3.loc[full_train_subtask3['stratify_group'].isin(single_instance_labels),\n",
        "                            'stratify_group'] = most_frequent_label\n",
        "\n",
        "train, val = train_test_split(full_train_subtask3, test_size=0.2,\n",
        "                              stratify=full_train_subtask3['stratify_group'],\n",
        "                              random_state=42)\n",
        "\n",
        "train = train.drop(columns=['label_sum', 'stratify_group'])\n",
        "val = val.drop(columns=['label_sum', 'stratify_group'])\n",
        "train.head()"
      ],
      "metadata": {
        "id": "-4Ur3wKFL3Ya"
      },
      "id": "-4Ur3wKFL3Ya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, labels= None, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False,\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_tensors='pt'\n",
        "                                  )\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # Only add labels if they are provided\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item"
      ],
      "metadata": {
        "id": "Qs-UjVYsInpD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Qs-UjVYsInpD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setup"
      ],
      "metadata": {
        "id": "w0vyIUEbMBu7"
      },
      "id": "w0vyIUEbMBu7"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(),\n",
        "                                    tokenizer,\n",
        "                                    train[['vilification',\n",
        "                                           'extreme_language',\n",
        "                                           'stereotype',\n",
        "                                           'invalidation',\n",
        "                                           'lack_of_empathy',\n",
        "                                           'dehumanization'\n",
        "                                           ]].values.tolist()\n",
        "                                    )\n",
        "\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(),\n",
        "                                  tokenizer,\n",
        "                                  val[['vilification',\n",
        "                                       'extreme_language',\n",
        "                                       'stereotype',\n",
        "                                       'invalidation',\n",
        "                                       'lack_of_empathy',\n",
        "                                       'dehumanization'\n",
        "                                       ]].values.tolist()\n",
        "                                  )\n",
        "\n",
        "dev_dataset = PolarizationDataset(dev['text'].tolist(),\n",
        "                                  tokenizer\n",
        "                                  )"
      ],
      "metadata": {
        "id": "-yxSaDCA9IMi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-yxSaDCA9IMi"
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           num_labels=6,\n",
        "                                                           problem_type=\"multi_label_classification\"\n",
        "                                                           )"
      ],
      "metadata": {
        "id": "0VXqqqIH9A3M"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0VXqqqIH9A3M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1\n",
        ")\n",
        "\n",
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    if isinstance(p.predictions, tuple):\n",
        "        logits = p.predictions[0]\n",
        "    else:\n",
        "        logits = p.predictions\n",
        "\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(logits))\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}"
      ],
      "metadata": {
        "id": "QLubGtx988hm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QLubGtx988hm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "IRsEWWkkMDg1"
      },
      "id": "IRsEWWkkMDg1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")"
      ],
      "metadata": {
        "id": "qEhm1TEv82mP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qEhm1TEv82mP"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "qdh-RgVHwPlb"
      },
      "id": "qdh-RgVHwPlb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 3: {eval_results['eval_f1_macro']}\")"
      ],
      "metadata": {
        "id": "_XM48q0BRBdo"
      },
      "id": "_XM48q0BRBdo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "Vt1G9HclL_Vq"
      },
      "id": "Vt1G9HclL_Vq"
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dev_3_dataset = PolarizationDataset(eng_dev_3['text'].tolist(), tokenizer)\n",
        "hau_dev_3_dataset = PolarizationDataset(hau_dev_3['text'].tolist(), tokenizer)\n",
        "\n",
        "output_dir_subtask3 = 'subtask_3'\n",
        "os.makedirs(output_dir_subtask3, exist_ok=True)\n",
        "\n",
        "original_label_order_in_preds = ['vilification',\n",
        "                                 'extreme_language',\n",
        "                                 'stereotype',\n",
        "                                 'invalidation',\n",
        "                                 'lack_of_empathy',\n",
        "                                 'dehumanization'\n",
        "                                 ]\n",
        "\n",
        "desired_output_label_order = ['stereotype',\n",
        "                              'vilification',\n",
        "                              'dehumanization',\n",
        "                              'extreme_language',\n",
        "                              'lack_of_empathy',\n",
        "                              'invalidation'\n",
        "                              ]\n",
        "\n",
        "label_to_pred_index = {name: original_label_order_in_preds.index(name) for name in original_label_order_in_preds}\n",
        "\n",
        "# --- Generate predictions for English ---\n",
        "print(\"Generating predictions for English (Subtask 3)...\")\n",
        "predictions_eng_3 = trainer.predict(eng_dev_3_dataset)\n",
        "probs_eng_3 = torch.sigmoid(torch.from_numpy(predictions_eng_3.predictions[0]))\n",
        "predicted_labels_eng_3 = (probs_eng_3 > 0.5).int().numpy()\n",
        "ids_eng_3 = eng_dev_3['id'].tolist()\n",
        "\n",
        "submission_df_eng_3 = pd.DataFrame({'id': ids_eng_3})\n",
        "for col_name in desired_output_label_order:\n",
        "    pred_idx = label_to_pred_index[col_name]\n",
        "    submission_df_eng_3[col_name] = predicted_labels_eng_3[:, pred_idx]\n",
        "\n",
        "output_filename_eng_3 = os.path.join(output_dir_subtask3, 'pred_eng.csv')\n",
        "submission_df_eng_3.to_csv(output_filename_eng_3, index=False)\n",
        "\n",
        "print(f\"English (Subtask 3) predictions saved to {output_filename_eng_3}\")\n",
        "print(submission_df_eng_3.head())\n",
        "\n",
        "# --- Generate predictions for Hausa ---\n",
        "print(\"Generating predictions for Hausa (Subtask 3)...\")\n",
        "predictions_hau_3 = trainer.predict(hau_dev_3_dataset)\n",
        "probs_hau_3 = torch.sigmoid(torch.from_numpy(predictions_hau_3.predictions[0]))\n",
        "predicted_labels_hau_3 = (probs_hau_3 > 0.5).int().numpy()\n",
        "ids_hau_3 = hau_dev_3['id'].tolist()\n",
        "\n",
        "submission_df_hau_3 = pd.DataFrame({'id': ids_hau_3})\n",
        "for col_name in desired_output_label_order:\n",
        "    pred_idx = label_to_pred_index[col_name]\n",
        "    submission_df_hau_3[col_name] = predicted_labels_hau_3[:, pred_idx]\n",
        "\n",
        "output_filename_hau_3 = os.path.join(output_dir_subtask3, 'pred_hau.csv')\n",
        "submission_df_hau_3.to_csv(output_filename_hau_3, index=False)\n",
        "\n",
        "print(f\"Hausa (Subtask 3) predictions saved to {output_filename_hau_3}\")\n",
        "print(submission_df_hau_3.head())"
      ],
      "metadata": {
        "id": "NZzI8N8_K7Yo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NZzI8N8_K7Yo"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content && zip -r /content/subtask_3.zip subtask_3\n",
        "files.download(\"/content/subtask_3.zip\")"
      ],
      "metadata": {
        "id": "gAH4_A4wM5nF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gAH4_A4wM5nF"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezFgPOeOSAac"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ezFgPOeOSAac"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "preprocessing-section",
        "data-loading-section",
        "dataset-section",
        "training-section",
        "predictions-section",
        "next-steps",
        "Jn13x5HaFwJj",
        "Ds2F4DodGUvm",
        "1h--DDLOGYnD",
        "TC4rtWdeGfy1",
        "nOuDpDRjKeiq",
        "4JbPpW16NKFl",
        "KmgIhl4CLmX5",
        "8LWUIMqxLsaE",
        "P7s9cIXHLx20",
        "a1pLF773Ta0f",
        "y5eoy_biNQJx",
        "IRsEWWkkMDg1",
        "Vt1G9HclL_Vq"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b010f42a0ded455cac6eadaea335950e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5439313bb624917aba6721b533ee618",
              "IPY_MODEL_e1620306b3944d86b03fae64653780e8",
              "IPY_MODEL_d316a21bd39d44f9b37444a64ba28732"
            ],
            "layout": "IPY_MODEL_02e7cb047dc74117a84def9602962e76"
          }
        },
        "d5439313bb624917aba6721b533ee618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f32076955a49ed8bb81a689bd318b3",
            "placeholder": "​",
            "style": "IPY_MODEL_5ad099d97e4e4d458e7123c2f8938b85",
            "value": "tokenizer_config.json: "
          }
        },
        "e1620306b3944d86b03fae64653780e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6da78ae3d808410999ed534816653d11",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd19c38fa864496ab5146cfbc36a15d1",
            "value": 1
          }
        },
        "d316a21bd39d44f9b37444a64ba28732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4719033a9604646b3b64434c002cdc8",
            "placeholder": "​",
            "style": "IPY_MODEL_03783b0611b64bb78c0046731a0988bf",
            "value": " 1.93k/? [00:00&lt;00:00, 60.7kB/s]"
          }
        },
        "02e7cb047dc74117a84def9602962e76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f32076955a49ed8bb81a689bd318b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ad099d97e4e4d458e7123c2f8938b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6da78ae3d808410999ed534816653d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bd19c38fa864496ab5146cfbc36a15d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4719033a9604646b3b64434c002cdc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03783b0611b64bb78c0046731a0988bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e7481accddc41c288d3db17a236253d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efd605c735044364a1e6c8948e2fe8a7",
              "IPY_MODEL_a681b405bdda4c8fa8df458300a5bb33",
              "IPY_MODEL_851a6976d933476883bdf8bee69edc27"
            ],
            "layout": "IPY_MODEL_0e59f053cff846fda165ca48b3b2ec51"
          }
        },
        "efd605c735044364a1e6c8948e2fe8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05a0b9a7b92348cebe202c6023bf292f",
            "placeholder": "​",
            "style": "IPY_MODEL_ef145c90d8254731a9e1333c3f8e7750",
            "value": "tokenizer.json: "
          }
        },
        "a681b405bdda4c8fa8df458300a5bb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13b66da517a24776b77869de7f9a977c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbb202f2ffcf46b5b2513f7bd1551a33",
            "value": 1
          }
        },
        "851a6976d933476883bdf8bee69edc27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f85b4167db2548e9812025efaecf6f7e",
            "placeholder": "​",
            "style": "IPY_MODEL_894d0f08375245718123f75688e851f4",
            "value": " 1.33M/? [00:00&lt;00:00, 14.1MB/s]"
          }
        },
        "0e59f053cff846fda165ca48b3b2ec51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a0b9a7b92348cebe202c6023bf292f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef145c90d8254731a9e1333c3f8e7750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13b66da517a24776b77869de7f9a977c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bbb202f2ffcf46b5b2513f7bd1551a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f85b4167db2548e9812025efaecf6f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894d0f08375245718123f75688e851f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b37a43ec89e64f708e208e0810e7c087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c5d051e8d4f44769ddae91cd87ab533",
              "IPY_MODEL_b41c339e99dc4368a113411ab06921bc",
              "IPY_MODEL_043b4a7ed4ec43b786e9b78eed62f5e1"
            ],
            "layout": "IPY_MODEL_20ab0079814b4d5fb58d56c8cdd13c1e"
          }
        },
        "6c5d051e8d4f44769ddae91cd87ab533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd850f82d86d4cf695f437ccc9ec5eba",
            "placeholder": "​",
            "style": "IPY_MODEL_0aec14054c1b44069e97e6ccf9aa7698",
            "value": "special_tokens_map.json: "
          }
        },
        "b41c339e99dc4368a113411ab06921bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa923e90a5a44b7789d8d27e5aa92370",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb7720a9a9e74f0eb54793722c2a6f0d",
            "value": 1
          }
        },
        "043b4a7ed4ec43b786e9b78eed62f5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1120a1d0325434da63608566d131416",
            "placeholder": "​",
            "style": "IPY_MODEL_bdf4ce23b3e34047be77c25b6f46b641",
            "value": " 1.79k/? [00:00&lt;00:00, 28.5kB/s]"
          }
        },
        "20ab0079814b4d5fb58d56c8cdd13c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd850f82d86d4cf695f437ccc9ec5eba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aec14054c1b44069e97e6ccf9aa7698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa923e90a5a44b7789d8d27e5aa92370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fb7720a9a9e74f0eb54793722c2a6f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1120a1d0325434da63608566d131416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf4ce23b3e34047be77c25b6f46b641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eb023ee51e2418d9b034a92626cd4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd1db27720ff4f759b29decfc3382591",
              "IPY_MODEL_a097219bbfd047e9bdf73c833819080a",
              "IPY_MODEL_91a38cdcb47d4bb29bd9e28c90298d1c"
            ],
            "layout": "IPY_MODEL_ee46777b90d445a78b39b7adb5510832"
          }
        },
        "cd1db27720ff4f759b29decfc3382591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8a8743b02c84269b38d7ac34ee8d91b",
            "placeholder": "​",
            "style": "IPY_MODEL_182c4505fc334ffcb0c146bf0fd775fd",
            "value": "config.json: "
          }
        },
        "a097219bbfd047e9bdf73c833819080a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9559449b53b2445aafa7b2e5876c2c87",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13046cf8f4434cc7a6132edf7448b551",
            "value": 1
          }
        },
        "91a38cdcb47d4bb29bd9e28c90298d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8b6f5f20d64da58a6eb8d17f25b00a",
            "placeholder": "​",
            "style": "IPY_MODEL_be26631554dd4ec7b521eb97de000db8",
            "value": " 1.38k/? [00:00&lt;00:00, 53.8kB/s]"
          }
        },
        "ee46777b90d445a78b39b7adb5510832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a8743b02c84269b38d7ac34ee8d91b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "182c4505fc334ffcb0c146bf0fd775fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9559449b53b2445aafa7b2e5876c2c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "13046cf8f4434cc7a6132edf7448b551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd8b6f5f20d64da58a6eb8d17f25b00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be26631554dd4ec7b521eb97de000db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d89446f0d174ae4b03584a9d77f858c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f93e77a29ece439cb448bf3dfc163c23",
              "IPY_MODEL_b68a766b90fe42a28928267ec42f79dd",
              "IPY_MODEL_ac87bb0a0ffc45148719e3ecacdef1cb"
            ],
            "layout": "IPY_MODEL_cc9d77306dcb4a478b9a055fc43941cb"
          }
        },
        "f93e77a29ece439cb448bf3dfc163c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5694e063944b4973bb055582401019bf",
            "placeholder": "​",
            "style": "IPY_MODEL_57c7ba3a8c81487382ea585ca7b434bc",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "b68a766b90fe42a28928267ec42f79dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f79b46e45344f4874af3874e140c83",
            "max": 916189306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8059f7472c4a47d5838506aefd920aea",
            "value": 916189306
          }
        },
        "ac87bb0a0ffc45148719e3ecacdef1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fc6700a9bb844119018ed96427eabd9",
            "placeholder": "​",
            "style": "IPY_MODEL_27e3d4a08f1242e0b854ee7910099580",
            "value": " 916M/916M [00:13&lt;00:00, 75.0MB/s]"
          }
        },
        "cc9d77306dcb4a478b9a055fc43941cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5694e063944b4973bb055582401019bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57c7ba3a8c81487382ea585ca7b434bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f79b46e45344f4874af3874e140c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8059f7472c4a47d5838506aefd920aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fc6700a9bb844119018ed96427eabd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e3d4a08f1242e0b854ee7910099580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d28573c4bc4790a4302ea04e039163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dc3113e82e74f3fa2f3dffca27d41e3",
              "IPY_MODEL_85774f30196b495899f7115f4fdc8155",
              "IPY_MODEL_5083b3c5bfc84f459381b4d6aba346fd"
            ],
            "layout": "IPY_MODEL_f33470eb423b488f8c1f2965f557d118"
          }
        },
        "1dc3113e82e74f3fa2f3dffca27d41e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25b9b5c7e874410286a4d487c15c3fd0",
            "placeholder": "​",
            "style": "IPY_MODEL_f288799196bd4effa00e16ee4e7feffd",
            "value": "model.safetensors: 100%"
          }
        },
        "85774f30196b495899f7115f4fdc8155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794aa4259e304e90aefffa7761b7fe78",
            "max": 916144032,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1628470e25448e09f17ed72e374771b",
            "value": 916144032
          }
        },
        "5083b3c5bfc84f459381b4d6aba346fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727aae7705a944639031260aca12f8e1",
            "placeholder": "​",
            "style": "IPY_MODEL_6079cda4b50144a78f56c4a894b02ea0",
            "value": " 916M/916M [00:12&lt;00:00, 122MB/s]"
          }
        },
        "f33470eb423b488f8c1f2965f557d118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b9b5c7e874410286a4d487c15c3fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f288799196bd4effa00e16ee4e7feffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "794aa4259e304e90aefffa7761b7fe78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1628470e25448e09f17ed72e374771b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "727aae7705a944639031260aca12f8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6079cda4b50144a78f56c4a894b02ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}